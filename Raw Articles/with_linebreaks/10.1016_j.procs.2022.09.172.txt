Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 207 (2022) 1164–1173


26th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2022)
Safety Navigation using a Conversational User Interface For Visually Impaired People
Madalin Mateia, Lenuta Alboaiea, Adrian Iftenea
aFaculty of Computer Science, "University Alexandru Ioan Cuza", Iasi

Abstract
Starting with everyday activities such as taking a walk at the outside environments, use of different public transportation methods or even buying different goods, visually impaired people need to put in a considerable effort to complete these so-called trivial tasks for sighted people. Given this, according to WHO (World Health Organization) it is estimated that there are at least 2.2 billion people who have a near or distance vision impairment. This paper presents an ongoing project that aims to address mainly two problems: the creation of a system designed to safely moving visually impaired people to points of interest using public transportation and designing of a communication mechanism that will balance between keeping the individual as safe as possible and also providing relevant indications. The current paper is divided in two main parts. In the first part, in addition to the study of similar systems, the factors underlying the architectural decisions on which the developed system was built, are debated and analysed. The second part presents the results obtained after a series of preliminary tests carried out with the help of visually impaired people. The application seems promising, being enthusiastically received by those who tested it, because it offers them safety and independence when travelling through the city.
© 2022 The Authors. Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)
Peer-review under responsibility of the scientific committee of the 26th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2022)
Keywords: visually impaired people, EOA, conversational user interface, human interaction ;



Introduction
People with some sort of visual disease are spread across all the world and although if it was hard until 90s to efficiently estimate how many they are and what are their needs due to projects like VISION initiated by WHO now we can have a better overview at them. According to [1] the Right to Sight tried to provide a better estimation of blind people which spans over 30 years (2050). They found out that there are around 43.3 million blind people. Besides these, which have severe visual impairment there are 295 million people with moderate or severe vision impairment,

E-mail address: matei.madalin@info.uaic.ro



1877-0509 © 2022 The Authors. Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)
Peer-review under responsibility of the scientific committee of the 26th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2022)
10.1016/j.procs.2022.09.172

this category includes people which can perceive light from different sources. The forecast includes estimation until 2050 and the number of fully blind people (no visual acuity) will raise up to 61 million and 474 million people will be affected by a moderate or severe vision impairment.
At the national level, the statistics are in line with those presented above. According to National Statistics [2], out of a total of 900,000 people with disabilities, approximately 84,000 are visually impaired and therefore fall into this category. In the city of Iasi, where we tested and developed the system, there are roughly 3000 visually impaired people.
The main research questions that we will address further in the paper are: Can we create an application for the visually impaired to help them navigate to a point of interest also using public transportation methods? How we can provide feedback about the path in a way in which we have a balance between keeping the individual as safe as possible and also providing relevant indications?.


Conversational User Interfaces

The IT field of smart assistants has seen a massive increase in recent years due to the advancement of technologies in the field of Artificial Intelligence (AI) and also because of increasing the number of visually impaired people (Section 1). Such an opportunity will be a perfect way to replace traditional and already outdated graphical user interfaces because this method of interaction started to gain popularity due to the fact that it can provide users with information, respond to different questions but more important it can assist them in the control of other connected electronics. It is estimated that the number virtual assistant will surpass 8 billion people (which is almost equal to the current world's population) [3].
At the base, a conversational interface describes any form of entity used to interact via text, here referring to chat bots or by voice processing, including voice assistants. Voice based assistants aim at mimicking human behavior as much as possible and in a simpler way because the users have the power to control the order of the conversation and its content compared to GUIs (Graphical user interfaces) where most of the same information is arranged the same for everyone [4]. Another characteristic worth to be mentioned is the fact that now, the applications become customized for the user from the tone, emphasis even to the speed of speech, and due to this the users who use such technology perceive the exchange of remarks with the assistant in a more personal way. Hence, the idea that those who use such interfaces end up thinking about them more like friends or close friends and less than simple computers by adding them essential human capacities and traits such as personality, feelings or rational thought [5]. A good example of such system that is currently used by blind people for navigation is BlindSquare [6] which provides navigation guidelines in English through voice synthesis.


Electronic orientation aids

As the number of blind and partially sighted people has been increasing, developers and researchers came with new technologies that tried to fulfill their needs and to help them in day by day activities. EOAs (Electronic orientation aids) purpose is to give orientation and way finding information to people affected by visual impairments.
Starting with the white long cane devices which were able to guide people on short path and also notify them about possible obstacles, technology shifted towards small devices with many sensors' capability (smart phones). If in the past, not sighted people had to carry with them a big device, sometimes heavy, that helped them to navigate in different environments, today the same capability can be obtained through smart phones [7]. Our focus has been on these technologies because they are well suited in unknown environments and for difficult routes that are not used frequently.
The problem regarding this orientation aid in general is that it provides guidance step by step and in a way that it is hard for a blind person to create a mental representation of the space [8]. As we will show through the preliminary tests, the problem of this space representation will be solved through a conversational speech based user interface that is built using studied design considerations.

Preliminary discussion and identified problems

In order to correlate existing literature with the different difficulties that a blind person may face in daily tasks, we conducted a preliminary discussion with the Blind Association from our city. We wanted to validate our assumptions about the problems a blind person may face, and what are their thoughts about these hypotheses. Other findings that resulted from this were: what other application they currently use in order to actively participate in society, the usability standards applied on these, how hard is to actually use a mobile application and the most predominant mobile platform.
Here, we have identified that use of public transport is the activity that they would avoid it the most because they need to invest a considerable amount of effort because of the lack of accessibility features.The lack of a digital panel where buses routes are shown, the font and sizes too small are just some problems that led to this lack of accessibility. Given these and the lack of digitization, it clearly that there is no way for them to find out what bus or tramway is coming.
Continuing, another topic of interest that resulted from discussions was the way they orient and navigate through the city in order. A red flag here is that they can hardly realize at all if they are nearby a pedestrian crossing, which often create confusions, and it threatens their safety.
The next section will present an overview of the studied domain, with relevant applications and projects built for blind people. Section 3 will present the proposed system which will act as a solution to the aforementioned gaps, in Section 4 we present some preliminary tests made in collaboration with the Association of Blind and how relevant this testing is for the system. In the end we will have a separate discussion section where we compare the solution with the analysed systems so far, the conclusions and some future directions.

Related work
Many of the applications built for people with visual impairments are built using designs from applications built for people who see normally. In [9] the authors present a portal where people with visual impairments can access mobile applications built especially for them. Google Assistant and Siri are intelligent agents that use speech recognition to answer questions, make recommendations, and perform actions by transferring requests to dedicated web services [10]. In [11], authors present how the blind people can access web applications using speech interactions, screen magnification or screen reader software. In [12] authors present a tool with the highest Optical Character Recognition (OCR) which was set up to help blind or visually impairment people when reading a book.
Additional to building special software systems like social media platforms [13], apps that are used in classrooms [14], apps for object recognition [15], apps for disable people interaction [16], special tablets with braille keyboards were created [17], as well as smart walking sticks based on ultrasonic technology that serves to detect obstacles to be faced by blind people while performing such activities as walking and running inside and outside the room [18], special interaction with health sensors [19].
In [20], the authors show that persons with visual impairments frequently use apps specifically designed for them to accomplish daily activities. Furthermore, this population is satisfied with existing mobile apps and would like to see more apps specially designed for them in the future. In the field of navigation, applications try to adapt to the needs of disabled people (suffering from various degrees of motor, visual and/or auditive impairment conditions) and come with new methods for route planning and navigation [21]. In [22], authors present how they create a mobile B-Help application that can help the blind people to communicate efficiently and to track locations. This application runs on a GPS equipped smartphone.
In the present, according to [23] study, it becomes apparent that people with disabilities are under-represented in the growth of mobile health software applications for smartphones (mHealth). After a multi-modal approach (literature review, Internet search, survey of disabled smartphone users), several areas of future research and development needed to support the inclusion of people with disabilities in the mHealth revolution are identified. One of them is related to the fact that the mHealth apps need to be designed to accommodate the accessibility needs of disabled users.
In [24] the author designed a system which is combines GPS, GSM and Bluetooth in order to facilitate the inte- gration between buses and with user's device. As a prerequisite condition, the public transportation vehicles need to have special electronic equipment. In order for a person to identify in which bus station he has, he needs to read its

Braille representation from a plate located there or he can manually ask for the location in the application using GPS. When a bus is approaching the station, the device will emit a sound to let the person know. Some problems identified in this approach are that the interaction between blind people and the system is facilitated through a graphical user interface which might create confusion for the blind. Also, the feedback is limited and provided only through different prerecorded sounds.
[25] states that blind people have reduced mobility and we shouldn't force them to adapt to our systems but rather adapt the systems to their needs. The author identify that if there are multiple buses in a station it can create confusion for a person because there will be no way for him to know which one is the right bus. A proposed solution for this is by integrating the smartphone with a device that is capable to emit sound from inside the bus. The person will be able to guide himself with the help of this sound and pick the right bus. In [26] the author is solving the interoperability problem by create a link between the blind and bus driver through a system that acts as a middleman between these two parties.

The proposed solution
This chapter is dedicated to the entire architecture of the application, along with the components and the interaction between them. Beside these various aspects of the technology used in different contexts as long as the cloud services (where it applies) will be presented. Moreover we will also address the following topics:

how the design of the application favors its use by people with disabilities;
data collection for later processing;
sensitive things that might impact the accessibility features like NFRs;
interaction methods with other smart/IoT devices such as Raspberry Pi.
cost analysis and ways to reduce it

Next we will focus on application architecture where we will discuss each component and its importance. At the architectural level, the application is structured on several microservices that communicate with each other through different communication protocols, the most common one being HTTP (check Fig. 1 for details). The reasons why we chose this architecture are related to the fact that the actual testing of each service can be done independently and very easily, the deployment can be done at the service level without having to rebuild the entire application when making changes at service level. Besides these, the aspect that we consider the most important is the possibility to scale each service independently and according to the degree of its load, unlike using a monolithic architecture when it may be necessary to allocate resources to the entire application and not just to one component [27].
The assisted guidance system denotes the core of the application, here being encapsulated all the logic of the application. Also, multiple integrations with different subassemblies are created here, such as: the sound processing
system, used for the recognition of the user's voice, including here the Romanian language as the language accepted in the application. In addition to the voice processing system, there is also integration with the data extraction service regarding the schedule and the public transportation routes in real time.
Besides integrations with different subsystems, at this level we incorporate independent cloud services, such as: Google Maps services for route and location details, Azure Blob Storage to save participant voice input, and Azure Cognitive which is used to synthesize voice and send voice feedback to the user.
Live Bus Data Service denotes the subassembly of the application responsible for collecting and processing the data that is used by the mobile application in order to notify users with alerts about public transport. As you can see
in Fig. 1, at this point, the system integrates with an external provider called "Here It Is" from where it will retrieve data related to bus itineraries. This application is very popular in Iasi, Romania because it offers an easy way to access public transport by indicating the position of each bus/tramway. Unfortunately, the accessibility features of the application are non-existent. At this point, the integration uses several cloud services, the external application calls an API (Azure Function) that will store the data in a database that is also part of the same ecosystem (Azure SQL). This data will be used later in the mobile application to notify users.
Sound Processor Service's purpose is to store and process the voice input that the user transmits to the mobile ap- plication. As a way of interaction, the mobile application will communicate with this service through HTTP requests,



Fig. 1. "The Helper" application architecture.


this being a service hosted in the Azure cloud through a PaaS service called App Service. The container integrates with Azure Blob to store each user record, the reason being that in the future we will need to post-process the data. On the speech recognition side, the system integrates with the Google Cloud Speech-To-Text service that will turn audio recording into text.
The Route Handler component represents the most important service that makes up the API (mobile application backend). It is based on specific Google APIs such as Directions and Places to calculate routes and extract information about locations around the user. Additionally, we have specific services that help manage the user and the preferential accessibility settings but also the Interest Location service which will persist data about some locations which the user frequents to be as easy as possible for him/ her to revisit that location. Another used service is the Compass service, through which the phone compass is accessed. Also, here, the data processing is done which involves the translation from degrees that indicate the magnetic north in Cartesian points. In order to assure the interaction between the user and this interface, we have the service that synthesizes the voice to send voice feedback to the impaired person. This service uses the Azure platform through Cognitive Service, which is a text-to-speech (TTS) module. In this service, the user is also alerted by text notifications, in addition to the voice, specific to the mobile device. The Loop Recorder Service is a way for the device to listen to the user's voice all the time and react only when the user addresses a specific command was created, for example "Where is the train station?". The reason why the service listens all the time is to diminish the interaction between the user and the application as much as possible by not forcing the user to press any button.
These are some of the commands that the user can send to the application via voice:

"Where is the X place?" The application will indicate the position in cardinal points.
"Get me there !" Told after the previous one initiates the travel mode and starts to indicate precise maneuvers to the user in order for him/ her to reach his/ her destination.
"What's on my left/right/around me?" This helps the user to discover interest objectives but also helps one to guide oneself.
"What bus/tramway is coming ?" This will indicate only the buses/tramways that are coming to the closest bus station related to the user's position.

"When bus/tramway X is coming?" This will indicate in how many minutes the X bus/tramway will arrive.
"What is the next station?" This will work only if the user is travelling by bus or tramway.


Fig. 2. The Helper" application. Simulation Mode.

We thought of an easy way of interaction between the people with visual impairments and IoT devices in order to notify them about the colour of the traffic lights. The principle of operation is as follows: the mobile application "The Helper" will connect to a smart Bluetooth device such as a Beacon that will be attached to the traffic light. When the user is in the proximity of the traffic light (and implicitly near the Beacon) the application will connect automatically and will receive data from it. In the communication protocol between the beacon and the mobile application we managed to transmit data through which we simulate the colours of the traffic light, implicitly 0 for red and 1 for green.
In order to simulate such an interaction, a device with Bluetooth capabilities from the IoT sphere was needed, and this was a Raspberry Pi. During simulation mode (see Fig. 2), a Raspberry Pi will act exactly like a traffic light that will send signals about its colour. As it can be seen, pedestrian crossings are marked with a specific icon. The simulator will wait for the Raspberry Pi device to send a signal that will result in a green traffic light, otherwise the simulator will not move forward.
The costs of our system come mainly from operational costs (OPex) summing up the costs for the platform to perform daily tasks. A large part of these operating costs come from the services offered by the Cloud providers we integrate with: Google Cloud and Microsoft Azure. The following costs are estimated based on the following scenario: the projected number of users are using the system for 2 hours every day and they create 6 vocal interactions each having 5 seconds per day, on average. On Azure side, in order to integrate with public transportation company and to retrieve the data, we use Azure Function service. A cost related advantage related to this is that it offers 1 million executions per month, therefore we estimated that we can sustain 30 users with this free plan. In addition to this, part of the costs goes to data storage: user personal information and also various data used by the system (bus schedule, stations) and its cost is about $30 per month. Google Maps offers $200 credits each month for free and based on this we can sustain around 50 users per month. For each 100 new users, we need to pay $20 monthly. Ways we could reduce the cost of Map services would be to migrate to another service that offers similar functionalities at a lower price (MapBox, Openstreet). Google Speech To Text also have a free plan that is 60 minutes of audio input every

month. With this free plan, we can support approximately 4 users per month. To reach 30 users per month, a total of
$8 is needed, at a price of 0.004 for each 15 seconds of audio processing. About the same costs, we can also deduct for the Text To Speech service. The ways in which we could reduce the costs related to voice and / or text processing for the Romanian language are limited. For the best of our knowledge, only Google is offering the text to speech service for the Romanian language, which makes it the only solution. A possible solution that would reduce costs would be to create and train our own machine learning model that can solve the tasks mentioned above for input processing. This would reduce costs up to $0 if we integrated the model directly into the user's device, using different optimization techniques for deep learning on mobile devices [29].
In conclusion, with an average cost of $50 per 50 users per month, we paid special attention to this feature that directly influences the quality of our system. By using services that optimize the infrastructure (serverless computing, Azure Function) we tried to reduce as much as possible the costs of integrating the system with different components, but at the same time we showed that to reduce dependency on key services such as Speech To Text, intensive research is needed to create and train machine learning models that can solve above-mentioned tasks.

Usability testing
We performed usability tests and collected end-users opinions about The Helper application (from volunteering that are affected by one or many visual impairments' disease). A good application that is made specially for people with disabilities should confer them a higher degree of autonomy when they try to travel by themselves, but also it should be able to keep them as safe as possible.
Besides these two characteristics that are very important, another discussed topic was level of effort they had to put in order to use the system, and how the vocal notifications are delivered to the end user without disturbing him and without making it stop using the application. The reason for doing this testing and collecting the feedback is to see what can be improved and changed in the application in the future research.
Methodology

The conducted usability test consisted of four steps (1) a preliminary discussion with the president of the association of the blind people in our city where we talked about what are the biggest problems they face in day-to-day tasks, (2) after that we had an introduction about The Helper application and how we think it can help with their difficulties,
(3) followed by a series of tasks that people with visual impairments had to do with the help of the application and
(4) in the end the post-test survey. We instructed the participants to think out loud and express their thoughts during the test. After the task series that we communicated verbally to the participants, we gathered their assessment of the overall experience using the QUIS (The Questionnaire for User Interaction Satisfaction) scale [28]. The tasks that users performed covered the main options from The Helper application, and each session was different. There were cases when we performed tasks waiting and travelling by bus or tramway which could take up to 30 minutes, but also we had to freely move across crowded pedestrian areas which took in an average 15 minutes.
Participants

For evaluation purposes, we have collaborated with two volunteers with visual impairments who are members of the "Association of the blind" from our city. Their skills to use mobile applications were on average good, so both people have used or still use mobile applications that help them with their daily activities. The average age of the participants was 50 years. In specialized terms, the diagnosis of volunteers is ocular atrophy with hereditary transmission, respectively absolute blindness by metabolic atrophy. Both participants have reduced vision so that they can hardly distinguish shapes and use the mobile phone for example: they use applications for narrating the text on the screen or a magnifying glass. Both people claim that they travel with difficulty both with the means of transport where they often end up in the wrong bus and realize this because they have memorized the number of curves that the bus takes to reach a known destination. In addition, city orientation is a problem for volunteers, who travel almost exclusively with companions.
Another noteworthy situation is when the sunlight is very strong. Normally, when the sunlight is not at maximum intensity, the person with ocular atrophy can hardly distinguish shapes and objects (e.g. cars, buses). In the initial case,

the sunlight will block the person's vision at a rate of almost 100%, and he will not be able to see at all. In this case, it is almost impossible to distinguish between the person with ocular atrophy who could normally distinguish shapes and objects and the person with absolute blindness who does not receive any visual stimulus.
Performed tasks

The First two tasks are related to the first discussed issue, namely how to make public transport easier to use.

In this task the volunteers had to use The Helper application with a pair of headsets and had to wait for several buses and tramways to travel to the current destination. Here we looked at whether the volunteer could properly distinguish the bus number in the station where he was in a timely manner, and when the next bus arrived at the station, without interacting with the surrounding people.
After testing the application several times with multiple buses, we tried to see if the application automatically acknowledges that the volunteer is on the bus and will correctly indicate the stations that follow en route. Also, another important thing that we have observed is the interaction the user had with the application, this interaction should be minimal and the minimum effort required would be pressing a button when the person is in the bus station
The third task was to use the application to guide us in space and to discover the objectives of interest in the surroundings on a predefined route in our city. The route length is approximately 1 km. We started from the centre of the city, and we went to one of the points of interest in the area. The reason why we chose this route is the fact that it is an intensely trafficked area, it has pedestrian areas but also areas with many objectives that can be used to help the user navigate. In order to test the Raspberry Pi capabilities regarding crosswalk warnings, we attached it to a traffic light post. The things we followed here are the following: the autonomy that the volunteer has in moving towards the desired destination through the voice instructions received by the application, if he can distinguish the objectives around him and be guided by them and most importantly, if it indicates that it is going to reach a pedestrian crossing and if the user is receiving data from the Beacon

A very important aspect to consider is that the tests were done during the day, in April, when the sun at noon reaches its maximum intensity and in this case people experience reduced vision even more than usual.
Results

In this section our focus is on how the application works after the preliminary discussion in which we identified problems for blind people, the features that help them, how these are notified in different situations, the synthetic voice of the assistant, the help received while guiding, the way they transmit a voice impulse to the application, and ways in which the application can be more easily used by further modifications.
The results of this section will be divided as follows: positive feedback and implicitly the parts where volunteers believe that the application worked well and of course the constructive part where ideas for improvement are suggested but also the correction, if necessary, of current problems.
On the positive side:
The volunteers believe that considering the early stage of the application and considering it being tested for the first time by people with disabilities, it works in optimal parameters, on the other hand they believe that their opinions were heard during the preliminary discussion. It can be seen in the design of the application that follows their suggestions from that moment.
The participants were delighted to be alerted when they arrived at a station and also the fact that they are alerted about bus/tramway lines running through that station. Also, they were happy with the distinction between bus and tram stations made by the voice interface (because they do not coincide in most cases). They found it very practical that the application automatically detects when they get on the bus and indicates the bus they got on, but also the stations they will reach along the route. Here they found the notification mode very non-intrusive by the fact that the notifications were made in the bus travels a distance of at least 500 m, the goal being to keep a balance between the number of notifications and the time the user forgets which station is next.

About the synthetic voice used by the application, after we set it to a lower than normal speech speed, they said that it is acceptable and that all words can be distinguished, also the diction and tone were very good.
In the part of navigating the city, the users say that they found the usefulness of the guidance module by correctly indicating the objectives in their surroundings. Another thing to note here was that the application notified them when they were approaching a pedestrian crossing and this led to more safety and confidence in the fact that one day they could walk alone around the city.
In terms of constructive feedback and ways to improve the application:
Although the way they talk to the application to orient themselves in space by asking simple questions like "Where's the train station?" should be as easy as possible, participants found the problematic task of talking to the application, either by not remembering well the sentence, either by the fact that it lasted a long time and the application did not respond after a certain number of seconds.
Ways to improve have also been suggested in the way the user interacts with the application when he wants to know which bus is coming to the station. At the moment, the person in question must press a button on the application screen to detect the nearest bus/ tramway station to start the data collection process. Volunteers said it would be very helpful for this process to happen automatically when they are at a station. Their wish here was the following: they would like to leave the application launched, to launch it for example in the morning before leaving and to leave it on during the day. The moment they arrive at the station and stay there for a predefined period of time (suggestions would be 15 seconds) it means that they are waiting for a bus, so the process must begin. If they are just passing through the station, but do not intend to get on the bus, this module should not be activated.

Conclusions
Besides the research that includes the collection of relevant data on how people with disabilities use a conversational interface application, the paper also presents architectural models in the area of software engineering, models without which such applications can not be created. The chosen architectural model, independent microservices, led to the development of an almost mature application that can be used by end users.
The preliminary testing part of the application was given time and effort through various mechanisms such as: creating a simulator to test the behaviour of the application in cities but also writing unit tests to validate its logic. The impact that this testing approach had before it came to be used by volunteers in usability tests is very high, as evidenced by the very good feedback received from them. As we explained in the previous chapter, an application for people with disabilities that does not work optimally can induce panic, anxiety and depression. Finally, in addition to the testing part, the paper also showed how after a preliminary discussion with volunteers and identifying the problems they face, an application can be created to help certain people from the community in which we live.
Next, we want to gather volunteers with various affections in order to better understand how they use applications designed for them, such as the conversational interfaces, and whether the proposed solution is suitable for their needs. In addition to the diseases that result in blindness, we want to focus our efforts in the near future on other categories of people with disabilities, and understand what their problems are and how they use the applications. Some of these categories would be: users with mental health, autism, cognitive disabilities, dyslexia or learning difficulties, dexterity impairments or deaf impairment.
A field that is still in its infancy but in which we invest a lot of time is related to finding a way to adapt the conversational interface to the vocabulary of the blind depending on the area in which they are and / or the level of their education.

Acknowledgements
Data processing and analysis in this paper were partially supported by the Competitiveness Operational Programme Romania, under project SMIS 124759 - RaaS-IS (Research as a Service Iasi)

References
Trends in Prevalence of Blindness and Distance and near Vision Impairment over 30 Years: An Analysis for the Global Burden of Disease Study
Autoritatea Na¸tionala˘ pentru Protect,ia Drepturilor Persoanelor cu Dizabilita˘¸ti (ANPDPD) - http://anpd.gov.ro/web/transparenta/statistici/trimestriale/
Topic: Virtual Assistants https://www.statista.com/topics/5572/virtual-assistants/.
Murad, C.; Munteanu, C. Designing Voice Interfaces: Back to the (Curriculum) Basics. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
Van, P. M. M. E.; Pluymaekers, M.; Lemmink, J. G. A. M. Human-like Communication in Conversational Agents: A Literature Review and Research Agenda.
BlindSquare - https://www.blindsquare.com/
Whitney, G. The Use of Electronic Orientation and Mobility Aids by Blind and Partially Sighted People.
(1) Kammoun, S.; Mace´, M. J.-M.; Oriola, B.; Jouffrais, C. Toward a Better Guidance in Wearable Electronic Orientation Aids. In Human- Computer Interaction – INTERACT 2011;
(1) Sierra, J. S.; Togores, J. S. R. Designing Mobile Apps for Visually Impaired and Blind Users: Using Touch Screen Based Mobile Devices: IPhone/IPad. 2012, 47–52.
(1) Dobosz, K. Designing Mobile Applications For Visually Impaired People. In Visually Impaired: Assistive Technologies, Challenges and Coping Strategies; 2017
(1) Mankoff, J.; Fait, H.; Tran, T. Is Your Web Page Accessible?: A Comparative Study of Methods for Assessing Web Page Accessibility for the Blind
(1) Galarza, L.; Martin, H.; Adjouadi, M. Integrating Low-Resolution Depth Maps to High-Resolution Images in the Development of a Book Reader Design for Persons with Visual Impairment and Blindness.
(1) Social Media Platforms for Low Income Blind People in India — Proceedings of the 17th International ACM SIGACCESS Conference on Computers Accessibility
Conference on Computers Accessibility, Lisbon, Portugal (2015) Lewis, L. L.: iOS in the classroom: A guide to teaching students with visual impairments
Arati, K., Sayali, A., Sushanta, D., Harshata, A.: Object recognition in mobile phone applications for visually impaired users.
Abid, S., Abid, S.: Mobile Application for Disabled People
Alhussaini, H., Ludi, S., Leone, J.: An evaluation of AccessBraille: A tablet-based braille keyboard for individuals with visual impairments.
Wall, A.A., Kumar, D., Bhardwaj, A.: Ultrasonic stick for blind,
Milne, L.R., Bennett, C.L., Ladner, R.E.: The accessibility of mobile health sensors for blind users.
Griffin-Shirley, N., Banda, D.R., Ajuwon, P.M., Cheon, J., Lee, J., Park, H.R., Lyngdoh, S.N.: A Survey on the Use of Mobile Applications for People Who Are Visually Impaired
Karimanzira, D.,. Otto, P., Wernstedt, J.: Application of Machine Learning Methods to Route Planning and Navigation for Disabled People
Febriandi, D.R., Widiyani, A.Y., Abdullah, A.S., Warnars, H.L.H.S., Utomo, W.H.:. Mobile Smart Application B-Help for Blind People Com- munity.
(1) Jones, M.; Morris, J.; Deruyter, F. Mobile Healthcare and People with Disabilities: Current State and Future Needs.
(1) Markiewicz, M.; Skomorowski, M. Public Transport Information System for Visually Impaired and Blind People
(1) Bischof, W.; Krainz, E.; Dornhofer, M.; Ulm, M. NAVCOM - WLAN Communication between Public Transport Vehicles and Smart Phones to Support Visually Impaired and Blind People;
(1) Krainz, E.; Bischof, W.; Dornhofer, M.; Feiner, J. Catching the Right Bus - Improvement of Vehicle Communication with Bluetooth Low Energy for Visually Impaired and Blind People.
O. Al-Debagy and P. Martinek, "A Comparative Review of Microservices and Monolithic Architectures,"
Chin, J. P., Diehl, V. A. and Norman, K. L.: Development of an instrument measuring user satisfaction of the human-computer interface
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.