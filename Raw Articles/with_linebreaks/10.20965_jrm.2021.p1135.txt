https://doi.org/10.20965/jrm.2021.p1135
VIDVIP: Dataset for Object Detection During Sidewalk Travel


Paper:
VIDVIP: Dataset for Object Detection During Sidewalk Travel
Tetsuaki Baba
Tokyo Metropolitan University
6-6 Asahigaoka, Hino, Tokyo 191-0065, Japan E-mail: baba@tmu.ac.jp
[Received May 6, 2021; accepted August 21, 2021]


In this paper, we report on the "VIsual Dataset for Visually Impaired Persons" (VIDVIP), a dataset for obstacle detection during sidewalk travel. In recent years, there have been many reports on assistive tech- nologies using deep learning and computer vision tech- nologies; nevertheless, developers cannot implement the corresponding applications without datasets. Al- though a number of open-source datasets have been released by research institutes and companies, large- scale datasets are not as abundant in the field of dis- ability support, owing to their high development costs. Therefore, we began developing a dataset for outdoor mobility support for the visually impaired in April 2018. As of May 1, 2021, we have annotated 538,747 instances for 32,036 images in 39 classes of labels. We have implemented and tested navigation systems and other applications that utilize our dataset. In this study, we first compare our dataset with other general- purpose datasets, and show that our dataset has prop- erties similar to those of datasets for automated driv- ing. As a result of the discussion on the characteristics of the dataset, it is shown that the nature of the im- age shooting location, rather than the regional char- acteristics, tends to affect the annotation ratio. Ac- cordingly, it is possible to examine the type of location based on the nature of the shooting location, and to in- fer the maintenance statuses of traffic facilities (such as Braille blocks) from the annotation ratio.

Keywords: dataset, deep neural network, object detec- tion, visually impaired

Background and Purpose
This research represents our belief that computer vision technology using deep learning will be extremely impor- tant for the visually impaired (e.g., to move around in the open air alone), and we are developing a basic dataset for this purpose. One of the reasons why these technologies have become applicable for social implementation is the development of deep learning. Deep learning is used to tackle a variety of social problems, particularly those dif- ficult to solve using conventional techniques. In the field
ing and computer vision has been reported. Especially in computer vision technology, datasets play an impor- tant role in the effectiveness of algorithms, and in sup- porting application implementations. For example, when developing general-purpose object detection applications for people, cars, etc., ImageNet,1 Google's Open Im- ages Dataset [1], Microsoft's Common Objects in Context (COCO) [2], and PASCAL Visual Object Classes (VOC), etc. are well-known benchmark datasets. Thakurdesai et al. [3] proposed a dynamic obstacle (pedestrian, car, etc.) information system for outdoor pedestrians using the COCO and KITTI datasets [4], but the system was lim- ited to obstacle detection using generic labels. Our previ- ous studies and discussions [5, 6] have shown that Braille blocks, steps, planted areas, crosswalks, bicycle riders, push buttons for pedestrians, and handrails are useful for visually impaired persons walking outdoors. Although there are many reports on Braille block recognition meth- ods [7] and visual impairment assistive technologies [8] using various sensors, there are relatively few reports on the development of datasets for these technologies. In addition, in image recognition and object detection us- ing deep learning, both the object to be detected and the surrounding background affect the recognition accuracy. Therefore, to support the visually impaired in Japan, it is preferable for the image sets to be collected in Japan. In light of the above, when researchers and developers build computer vision-based applications to support the visually impaired in Japan, one of the barriers to imple- mentation is the need to create and develop the datasets themselves.
In our research, we have been developing a domestic- specific object detection dataset since April 2018; we are annotating it for object detection until March 2021, and for region segmentation from April 2021. At this time, for object detection, 538,747 instances have been regis- tered for 32,036 images. Fig. 1 shows examples of reg- istered images. Additional details regarding the develop- ment process up to December 2019 (at that time, the num- ber of registered images was 7,122 and the number of in- stances was 83,319) and the user experiments using the navigation system can be found in Refs. [9] and [10]. Our dataset is small compared to the 2.5 million instances in COCO [2] and the 9 million instances in the Open Images Dataset [1]; however, compared to the 27,450 instances
of assistive technology, research combining deep learn- 
https://image-net.org [Accessed September 26, 2021]
Journal of Robotics and Mechatronics Vol.33 No.5, 2021 1135
© Fuji Technology Press Ltd. Creative Commons CC BY-ND: This is an Open Access article distributed under the terms of the Creative Commons Attribution-NoDerivatives 4.0 International License (http://creativecommons.org/licenses/by-nd/4.0/).


Fig. 1. Examples of images actually annotated.


in the PASCAL VOC, it should be large enough to train a detection network.
The main focus of this research is the development of the dataset, and the specific objective is to release a trained model and dataset. This study examines the quality of the dataset and summarizes the differences from other general-purpose datasets, along with the characteristics of the dataset in each region in Japan. Examples of images that have actually been annotated are also provided.

Related Studies
In the implementation of this research, from the per- spective of moving on the sidewalk toward a destina- tion while avoiding obstacles, etc., we believe that the dataset for automated driving applications is similar to the format targeted for this research. At the beginning of our research in 2018, KITTI [4] was a well-known open dataset for automated driving applications, but in the 2020s, Waymo [11], DriveSeg [12], A2D2 [13], etc., were additionally released by major companies. KITTI has nine classes, including cars, pedestrians, and motor- cycles. Waymo has four classes, including cars, pedes- trians, signs, and bicyclists. DriveSeg has 12 classes in- cluding poles and vegetation, and A2D2 has 38 classes in- cluding buildings, traffic lights, crosswalks, and sidewalks (notably, A2D2 is for 3D object detection). Thus, the reg- istered class labels can vary depending on the dataset. By utilizing these existing datasets, it is possible to construct a detector for the purpose of this research at a certain level. However, in these datasets, the images are taken from the perspective of a car driver, not from that of a pedestrian, and the information required by the disabled party (such as steps, push buttons for pedestrians, and
Braille blocks) is not registered at all.
The dataset in this study is targeted at the detectors necessary for outdoor mobility (mainly on sidewalks) for the visually impaired, but can be applied to automated driving for wheelchairs, and in the near future, to safety sensing applications using headsets and transmissive glass displays. Ref. [14] designed a network based on Faster R-CNN by adding a distance sensor to a webcam, and an RGB+Depth channel for providing feedback regarding the surrounding information (people, cars, etc.) through voice navigation. Mulfari et al. [15] built a tensorflow en- vironment on a single-board PC (Rasberry Pi 3) and im- plemented a system for performing object detection us- ing a camera mounted on a pair of glasses; they also discussed the possibility of supporting the visually im- paired. Chaudhry et al. [16] developed a person identifi- cation support system for visually impaired people using face recognition.
Not only are there few publicly available datasets of pedestrians and wheelchair users moving around out- doors; moreover, very few trained models are available. In addition, even if the images are made public, there are other problems, such as the images not being designed for actual use. The former datasets can be considered as having problems related to the law. However, accord- ing to Article 47 of the Copyright Act, at least the lat- ter models are considered to be distributable under do- mestic law [17]. Thakurdesai et al. [3] developed an ob- ject detector for outdoor pedestrians using the general- purpose datasets COCO and KITTI. In Japan, Azuma et al. used GAN to calculate the path of an automated wheelchair [18] for independent movement, but they de- veloped their own dataset. Ohkita et al. also developed their own dataset to implement a navigation system for the visually impaired [19]. As the purpose of these studies

was not to develop datasets, not only have the datasets not been made public, but also their models are local learning models with low generalizability. As such, researchers who develop similar models in the future will not be able to receive the benefits of the dataset development from these studies.

Comparison with Representative Benchmark Datasets
The dataset developed here, i.e., the "VIsual Dataset for Visually Impaired Persons" (VIDVIP) dataset, was com- pared with the COCO and PASCAL VOC datasets (which are relatively similar in scale to our dataset) and with KITTI's three open datasets in terms of automated driving applications. In general, quantity is important for dataset quality and performance, but there are other metrics, such as the bounding box accuracy. For the benchmark of the remote sensing datasets, Li et al. [20] referred to the num- ber of instances, mean average precision (mAP), and box- plot as indicators of dataset quality, and we follow their discussion in this study.
The mAP is a measure used to indicate the accuracy of object detection and classification results in image recog- nition, and in 2D object detection, it represents the rate of matching between the bounding box of the training data and that of the recognition result. The mAP can be calcu- lated from the average of all classes using precision (the number of correctly predicted regions divided by the num- ber of all predicted regions) and recall (the number of correctly predicted regions divided by the number of all registered regions), where the AP is the area of the func- tion curve with recall on the horizontal axis and precision on the vertical axis. The mAP is also used as an indica- tor for benchmarking datasets such as PASCAL VOC and COCO; a detailed explanation can be found in Ref. [21].
By using a boxplot to show the data, we can see at a glance the average size and magnitude of each instance. This is meant not only to indicate the simple instance size in the image, but also to illustrate the nature of the dataset in more detail, by linking it to the shooting situation. For example, if the instance size related to cars is larger than usual, it implies that pedestrians are more likely to be walking next to or inside the parking lot. In addition, if the instance size of the pedestrian signal is large (as it is unlikely that the image is taken while approaching the pedestrian signal), this may suggest a human annotation error. By visualizing the instance size in this way, it is possible to improve the user's understanding of the situa- tion and the accuracy of the annotation process.
A summary list of each dataset is presented in Table 1. Table 2 shows a list of possible common class labels for

Table 1. Summary list of datasets to compare.

Datasets Class Image Instances mAP Year

VOC
20
17,125
23,979
83.68%
2012
COCO
80
82,081
255,403
55.3%
2014
KITTI
9
7,481
38,503
57.9%
2012
VIDVIP
39
32,036
538,747
70.43%
2021

Table 2. Comparison of common class labels.

Labels VIDVIP (ours) VOC COCO KITTI

Person
Bicycle Car
×
×
×
×
×
×
×
×
×
×
×
×
Motorbike
×
×
×

Bus
×

×
×
Train
×
×
×
×
Truck
×

×
×
Bicycler
×


×
Traffic light
×

×



Summary of "VIsual Dataset for Visually Impaired Persons" (VIDVIP) Dataset
There are two types of VIDVIP datasets, i.e., 2D ob- ject detection and semantic segmentation dataset. We are currently working on the semantic segmentation dataset, and this paper summarizes the 2D object detection of the former. There are 39 registered class labels; the selec- tion of these labels can be found in Ref. [9]. The top and bottom figures in Fig. 2 show the number of registered instances for each class label and the distribution of the registered area by boxplot, respectively. The vertical axis of the registration area distribution is the ratio of the reg- istered bounding box to the image size, and the closer the ratio is to 1.0, the more the annotation is done at the same size as the image.
For object detection, we used YOLOv3 [22] to check the accuracy. Fig. 3 shows the average precision of each class label and overall mAP. As noted above, 10% of the entire dataset was used as the test data for the calcula- tion. Table 1 shows a comparison of the mAPs for each dataset.2
Additional information on how the VIDVIP dataset was captured can be found in Ref. [9]. As the purpose of this study was to support sidewalk mobility in Japan, it was desirable to collect images from various regions. Ow- ing to geographical and experimental environmental con- straints, we decided to create a dataset of images from
this dataset and the VOC, COCO, and KITTI datasets. 
However, for KITTI, Van is read as "Car," Pedestrian and Person sitting as "Person," Cyclist as "Bicycler," and Tram as "Bus." In this study, the comparison is limited to 2D object detection.
For each mAP value, we referred to what follows.
VOC: https://paperswithcode.com/sota/object-detection-on-pascal-voc- 2007 [Accessed September 26, 2021]
COCO: https://pjreddie.com/darknet/yolo/ [Accessed September 26, 2021]
KITTI: https://github.com/packyan/PyTorch-YOLOv3-kitti [Accessed September 26, 2021]


Fig. 2. Number of registered instances per class label in the "VIsual Dataset for Visually Impaired Persons" (VIDVIP) dataset (top) and area distribution by boxplots (bottom).




Fig. 3. Accuracy list by YOLOv3. Each mean average pre- cision (mAP) is calculated on a 416 × 416 image size. How- ever, for KITTI, the image size is 320 × 320.
the following areas: Tokyo (Hino City, Musashino City, Hachioji City, Bunkyo-ku, Kita-ku), Kanagawa (Ebina City, Hiratsuka City), Aichi Prefecture (Kitanagoya City, Iwakura City, Inazawa City, Nagoya City, Kanie-cho, Toyokawa City), Hiroshima Peace Memorial Park (Hiro- shima City, Hiroshima Prefecture), and Kitakyushu City, Fukuoka Prefecture. The trends for each region and loca- tion are described below.
The annotation work has been continuously conducted in the research group by students skilled in this work and belonging to the author's laboratory. We developed our annotation application to work specifically on this dataset development. The annotated images were added to the dataset through an automatic decision algorithm based on a trained model and human visual check.

Comparison
PASCAL VOC is widely known as a relatively small benchmark dataset. For this comparison, we used the VOC2012 Challenge, which included the largest amount of data among the Pascal VOC Challenges. This dataset is related to recognition and object detection, and consists of 17,125 images and 23,979 instances annotated with 20 class labels, as shown in Table 1. The left figure in Fig. 4 is a boxplot of the bounding box area for each com- mon class label, where 1.0 indicates that the registered object is equal to the image size. All of the classes were photographed relatively large, but the area ratio of the car was smaller than that of the other instances. The right fig- ure in Fig. 4 shows the number of instances for each class

 

Fig. 4. Boxplots of bounding box area and number of in- stances per class for Visual Object Classes (VOC) dataset.
Fig. 6. Boxplots of bounding box area and number of in- stances per class for KITTI dataset (2012).


 

Fig. 5. Boxplots of bounding box area and number of in- stances per class for Common Objects in Context (COCO) dataset (2014).



label, and it can be seen that a person is registered more often than the others. Notably, the truck, traffic light, and bicycler numbers are zero, because these class labels are not included in the VOC dataset.
COCO is a benchmark dataset provided by Microsoft. The data used for this comparison were the training data from 2014. As of May 5, 2021, the main updating of this dataset stopped in 2017; the number of images had increased to approximately 120,000 as of 2017. Never- theless, for convenience, we used the data from the year 2014. However, the "Object Detection Task," an algo- rithm competition, was still held in 2020, indicating that this community remains very active. Fig. 5 shows the area ratio and number of instances. Both distributions are sim- ilar to those of the VOC dataset.
The purpose of KITTI is different from that of the VOC and COCO datasets, in that it is a dataset for automated driving. Annotations are made on images taken only from the perspective of the vehicle while driving. It is char- acterized by the availability of data from multiple sens- ing methods, such as stereo images and Laser Points of Clouds datasets. We used a 2D object detection dataset from this dataset. Fig. 6 shows the area ratio and num- ber of instances. The major difference between VOC and COCO is the small area ratio. It is thought that this is because the camera images are taken from inside the car while moving, so the object to be detected does not fill the entire screen. The number of instances of car is the high- est, followed by those of the person, bicycler, and bus. The zero instance indicates that there is no corresponding label.
In our VIDVIP dataset, the area ratio is expected to be smaller than that of the VOC and COCO datasets, as in

Fig. 7. Boxplots of bounding box area and number of in- stances per class for VIDVIP dataset.



KITTI, because the images are from the perspective of a moving pedestrian. Fig. 7 shows the area ratio and num- ber of instances for the VIDVIP dataset. It can be seen that the area ratio is slightly larger than that of the KITTI dataset. This is presumably because the distance between the object and photographer differs between images taken while walking and those taken while driving a car. The distribution of the number of instances is similar to that of the KITTI dataset. This indicates that the VIDVIP dataset has properties similar to datasets for automated driving applications, and that the objects to be detected are closer to the photographer than those captured from a car.

Region and Location
When creating a dataset, especially for image recogni- tion, it is important to consider the differences between regions and locations. In particular, because this research supports outdoor mobility for the visually impaired, the quality of the dataset should be carefully considered dur- ing development. In the future, as we continue to develop datasets, understanding the characteristics of regions and locations will enable us to judge the validity of registered instances, and the characteristics of the regions and loca- tions can be used as a basis for judging the selections of locations where image shooting needs to be performed in the future.
In this section, we discuss the regional and location characteristics of the VIDVIP dataset. Fig. 8 summarizes the annotation results (excerpts) for each shooting region. The reason why only Aichi Prefecture is not marked with cities and towns is that several images of cities and towns



Fig. 8. Ratio of registered instances in each class of the VIDVIP dataset. A value closer to 1.0 indicates that other classes are not registered (or few classes are registered).


were saved separately, and they are listed together as ref- erence data only. The horizontal axis represents the re- gion, and the vertical axis represents the ratio of instances of a given class label to the total number of instances. For example, the person instance of Musashino City, Tokyo, accounts for 10% of the entire Musashino City dataset. The ratio of person is outstanding (more than 10%) in Musashino City, Hiroshima Peace Memorial Park, and Aichi Prefecture. In terms of population density, Bunkyo- ku, Tokyo should show a similarly high figure, but it is less than 5%. This is because the value depends not only on the region, but also on the location where the images were taken. For example, Hiroshima Peace Memorial Park is a sightseeing spot, and there were many pedestri- ans on the day of the shooting. In Musashino City, Tokyo, most of the images were captured mainly in downtown Kichijoji. In the case of Aichi Prefecture, we were able to include images taken inside a railway station. More generally, the increase or decrease in the number of per- son instances is highly dependent on the shooting loca- tion. Similarly, Hiroshima Peace Memorial Park has a very small percentage of car instances, but this is because many of the images were taken in the park.
The percentage of bollards (which indicate car stops) was high in Hiratsuka City, Kanagawa. This may be ow- ing to the fact that the images were taken in an urban area where the roadway and sidewalk are relatively undivided by guardrails; as an alternative, there are several vehicle stops. The value of "guardrail" shows that the registra- tion ratio for Hiratsuka City is small as well. The fence and wall have a large registration rate in the data taken in Ebina City, Kanagawa, probably because they are im- ages of a residential area without sidewalks. Many images were taken of, e.g., the fence of a house or a field in the suburbs. These results indicate that the location, rather than the region, influences the percentage of the annotated class registration.
However, the regional characteristics do not necessarily affect the class registration rate. A signboard is a station- ary signboard on the eaves of a store, and is registered as an obstacle for visually impaired persons. For the same reason, a banner flag is registered as a flag. A certain
percentage of images in Hiratsuka City, Kanagawa, and Bunkyo-ku, Tokyo are registered accordingly. The im- age of Bunkyo-ku, Tokyo shows the route from the JR Ochanomizu Station to the Hongo Campus of The Uni- versity of Tokyo. In both regions, the images were taken while walking in the city center or in a shopping area, but the standing signboard and banner flag tend to be used more in Bunkyo-ku and Hiratsuka City, respectively. The number of such cases is small compared to the locality, but it is necessary to observe the image shooting location and region in combination.
The percentage of braille block images is extremely small in Ebina City, Kanagawa, because most of the im- ages were taken in residential areas. The percentage of registered Braille block images is also lower in Hiroshima Peace Memorial Park than in other places. In general, parks seem to be equipped with Braille block pavement, but our survey shows that Braille blocks were not in- stalled in the Atomic Bomb Dome of the Hiroshima Peace Memorial Park, and only partially in the park [23]. In contrast, in areas where there are many images of side- walks in the city, the registration rate is more than 15%. It can be confirmed that the registration ratio for steps in the Hiroshima Peace Memorial Park is remarkably high. This is because all tree plantings in the park are separated by stepped blocks. It can be confirmed that there are cer- tain steps in any location when moving outdoors. In the case of such a characteristic class registration ratio, it is possible to estimate its locality.
By comparing the developed datasets for each shoot- ing location, we were able to confirm the differences be- tween parks, urban areas, residential areas, and sidewalk routes. This information is useful for checking the qual- ity of the dataset. However, by combining this informa- tion with an object detector and simply walking along the specified route, one can utilize the occurrence frequency of Braille blocks and guardrails and statistics of the regis- tered dataset for public maintenance.
As described in Section 5, we are currently testing a smartphone application for improving the diversity of dataset images. By using this application, users can easily take pictures of areas with low recognition accu-

 
Fig. 9. Various press buttons for pedestrians registered in VIDVIP dataset.


racy on their way to school or while commuting routes (these pictures are assumed to be taken by caregivers or family members, not by the users themselves), and then send them to a data server. The transmitted images are assigned GPS coordinates, and it is assumed that user- specific dataset structures can be reconstructed by consid- ering regional and location characteristics in future model training. For example, if a user lives near a train sta- tion and his/her daily route is mainly through a shopping street, it would be better to not only reconstruct the dataset around the user based on GPS coordinates, but also to use dataset images with landscape characteristics similar to those around the shopping street for training. If the user mainly moves in a residential area, not only images of the region but also dataset images of landscapes similar to the residential area should be used for training.

Collection of Various Data
In this study, annotation work was conducted by small groups of approximately 5–6 people in our laboratory, or in collaboration with our collaborators. However, it is dif- ficult to conclude that the characteristics of the various re- gions have been completely covered. For example, push buttons for pedestrians vary slightly in shape and color, from the familiar yellow box type to acoustic push button and touch-type switches, as well as those for the elderly and other acoustic signals. Fig. 9 shows an example of the pedestrian push-button images registered in VIDVIP. In addition to different shapes and colors, minor differences in shapes, illustrations, and appearance changes owing to aging can be observed. As there exists the shape of a so-called "long push button" mounted on a long bar (see Fig. 10),3 it is desirable to register such data for the pur- pose of deep learning algorithms.
The collection of images in a wider variety of areas is a challenge, because pedestrian push buttons that deteri- orate over time, disappearing white lines and crosswalks, and damaged or decolorized Braille blocks cannot be eas-

Some image examples are presented in the article https://nonsensedances.com/?p=4376 where images of long push buttons of signals are collected (finally confirmed on May 5, 2021).
Fig. 10. Pop-out pedestrian push button called "long push button" (provided by a research collaborator).


Fig. 11. Example of Braille blocks not buried in the sidewalk.



ily generated by data augmentation. Therefore, our re- search group is currently developing an application for smartphones, as a system for easily providing images for annotation from all over Japan. The application runs on iOS and has already been tested as a beta version. Using a trained model, the object detection result can be checked in real time, and if the recognition status is not good, the image can be immediately uploaded to a cloud server on the Google Cloud Platform. The project team is currently testing the system and repeatedly uploading data for the parts of concern. Annotations can also be made from the smartphone and GPS coordinates at the time of shoot- ing, as well as the shooting direction and posture of the smartphone; these are saved as json data. Through con- ferences and events, we aim to create an environment in which many general users can easily participate in the de- velopment of this application.
In the course of collecting a large amount of image data, it became apparent that there were areas of concern from a safety viewpoint. Fig. 11 shows an image of a city taken around a station in Kita-ku, Tokyo. On this one- way street, we see a road for automobiles on the right and the sidewalk on the left. However, the Braille blocks are installed on the road, and it is unclear whether it is safe for the visually impaired to walk along the Braille blocks in this case. Other similar images that are difficult to un- derstand from a safety viewpoint have been reported, and

whether or not to annotate such images remains a future problem.

Future Prospects
In this study, the accuracy and quality of datasets de- veloped over a period of three years were compared with those of other benchmark datasets, and were examined and discussed in terms of regional and location character- istics. In the future, we will continue to collect images from the perspectives of pedestrians in various parts of Japan through smartphone applications. The trained mod- els are already available on the website in mlmodel and weights formats.4 In addition, we have started annotation work using semantic segmentation as well as object detec- tion, and we plan to provide datasets and training models for automated driving and safety control of wheelchairs and personal mobility devices. As the dataset created in this study is a set of images taken by the development team, we cannot make the dataset public, owing to privacy concerns (such as in regards to reflections of people). We are currently working on a research project with legal ex- perts to resolve this issue by preparing terms of use for the provision of images via smartphone applications.
Image recognition is already an indispensable technol- ogy element in VR, AR, and robotics developments. Nev- ertheless, it is difficult to develop unique applications such as object detection and semantic segmentation by sim- ply using generic datasets. We have already conducted a demonstration experiment with nearly 100 people using a navigation application based on our dataset, and we are also attempting to update map information based on the object detection information. We expect that this dataset and learning model will be used in various applications in the future.

Acknowledgements
This work was supported by JSPS Grants-in-Aid for Scientific Re- search JP18H03486. The development of the navigation system was subsidized by the Ministry of Health, Labour and Welfare of Japan for the development of equipment to support the indepen- dence of persons with disabilities.


References:
A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont- Tuset, S. Kamali, S. Popov, M. Malloci, T. Duerig, and V. Ferrari, "The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale," Int. J. of Com- puter Vision, Vol.128, pp. 1956-1981, 2020.
T.-Y. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick,
J. Hays, P. Perona, D. Ramanan, P. Dolla´r, and C. L. Zitnick, "Mi- crosoft COCO: common objects in context," CoRR, abs/1405.0312, 2014.
N. Thakurdesai, A. Tripathi, D. Butani, and S. Sankhe, "Vision: A deep learning approach to provide walking assistance to the visually impaired," CoRR, abs/1911.08739, 2019.
A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, "Vision meets robotics: The kitti dataset," Int. J. of Robotics Research (IJRR), 2013.
T. Baba, H. Watanave, and T. Kamae, "Design and prototyping for an outdoor activity support system for the visually impaired using deep learning for object detection," SIG Technical Reports, IPSJ, Vol.32018-AAC-7, No.8, Aug. 2018 (in Japanese).
K. Ishisone, T. Baba, H. Watanave, and T. Kamae, "Basic study and prototyping of an object detection dataset for outdoor mobility support for the visually impaired," SIG Technical Reports, IPSJ, Vol.2018-AAC-7, No.9, Aug. 2018 (in Japanese).
K. Kuzume, H. Masuda, and Y. Murakami, "Automatic identifica- tion of braille blocks by neural network using multi-channel pres- sure sensor array," 2020 The 3rd Int. Conf. on Computational Intel- ligence and Intelligent Systems (CIIS 2020), pp. 93-99, New York, NY, USA, 2020.
S. Asad, B. Mooney, I. Ahmad, M. Huber, and A. Clark, "Object detection and sensory feedback techniques in building smart cane for the visually impaired: An overview," Proc. of the 13th ACM Int. Conf. on PErvasive Technologies Related to Assistive Environ- ments (PETRA '20), New York, NY, USA, 2020.
T. Baba, K. Ishisone, K. Watanabe, H. Watanave, T. Kamae, K. Suematsu, S. Takata, and Y. Kuga, "Developing a localized ob- ject detection dataset supporting sidewalk use for visually impaired persons in japan," Trans. of the Virtual Reality Society of Japan, Vol.25, No.3, pp. 185-195, 2020.
T. Baba, "Design for the visually impaired when traveling out- doors using omnidirectional imagery and image recognition," Im- pact, Vol.2020, No.7, pp. 34-36, 2020.
P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z. Chen, and D. Anguelov, "Scalability in perception for autonomous driving: Waymo open dataset," Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recogni- tion (CVPR), Jun. 2020.
L. Ding, J. Terwilliger, R. Sherony, B. Reimer, and L. Fridman, "Mit driveseg (manual) dataset," IEEE Dataport, doi: 10.21227/mmke- dv03, 2020.
J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh, A. S. Chung, L. Hauswald, V. H. Pham, M. Mu¨hlegg, S. Dorn, T. Fer- nandez, M. Ja¨nicke, S. Mirashi, C. Savani, M. Sturm, O. Vorobiov,
M. Oelker, S. Garreis, and P. Schuberth, "A2D2: Audi Autonomous Driving Dataset," arXiv:1911.08739, 2020.
B. Kaur and J. Bhattacharya, "A scene perception system for vi- sually impaired based on object detection and classification using multi-modal DCNN," CoRR, abs/1805.08798, 2018.
A. Palla, D. Mulfari, and L. Fanucci, "Using tensorflow to design assistive technologies for people with visual impairments," IADIS Int. Conf. Big Data Analytics, Data Mining and Computational In- telligence 2017 (part of MCCSIS 2017), pp. 110-116, 2017.
S. Chaudhry and R. Chandra, "Design of a Mobile Face Recog- nition System for Visually Impaired Persons," arXiv:1502.00756, Feb. 2015.
Information Technology Promotion Agency, "AI Hakusho2020," 2020 (in Japanese).
D. Azuma et al., "Development of the new welfare service "partner mobility" using ai interactive automated drive system," Bulletin of Kurume Institute of Technology, Vol.43, pp. 2-12, Mar. 2021 (in Japanese).
T. Okita, H. Kojima, S. Ooi, and M. Sano, "A study on naviga- tion system for visually impaired person based on egocentric vision using deep learning," Technical Report 10, Graduate School of In- formation Science and Technology, Osaka Institute of Technology; College of Information Science and Engineering, Ritsumeikan Uni- versity; Faculty of Information Science and Technology, Osaka In- stitute of Technology, Mar. 2019 (in Japanese).
K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, "Object detec- tion in optical remote sensing images: A survey and a new bench- mark," ISPRS J. of Photogrammetry and Remote Sensing, Vol.159,
pp. 296-307, Jan. 2020.
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and
A. Zisserman, "The pascal visual object classes (voc) challenge," Int. J. of Computer Vision, Vol.88, No.2, pp. 303-338, Jun. 2010.
J. Redmon and A. Farhadi, "Yolov3: An incremental improvement," arXiv:1804.02767, 2018.
K. Watanabe, T. Baba, K. Tamura, H. Watanave, and T. Kamae, "Basic study of support for visually impaired people touring monu- ments in hiroshima peace memorial park," SIG Technical Reports, IPSJ, Vol.2018-AAC-8, No.2, Nov. 2018 (in Japanese).


https://tetsuakibaba.jp/project/vidvip [Accessed September 26, 2021]



Name:
Tetsuaki Baba

Affiliation:
Tokyo Metropolitan University




Address:
6-6 Asahigaoka, Hino, Tokyo 191-0065, Japan
Brief Biographical History:
2009 Received Ph.D. degrees in Art and Technology from Kyushu University
2013- Associate Professor, Tokyo Metropolitan University
Main Works:
Freqtric Drums (https://freqtric.com)
Interaction design and media art
Product design and media art, such as musical instruments, experimental devices, gadgets, educational materials, etc.
Membership in Academic Societies:
Information Processing Society of Japan (IPSJ)
Association for Computing Machinery (ACM)
Asia Digital Art and Design Association (ADADA)