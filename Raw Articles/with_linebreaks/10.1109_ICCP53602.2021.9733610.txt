Pothole Detection for Visually Impaired Assistance

Stefan-Daniel Achirei Computer Engineering Department Technical University of Iasi
Iasi, Romania
stefan-daniel.achirei@academic.tuiasi.ro
Ioana-Ariana Opariuc
Computer Engineering Department Technical University of Iasi
Iasi, Romania
ioana-ariana.opariuc@student.tuiasi.ro
Otilia Zvoristeanu
Computer Engineering Department Technical University of Iasi
Iasi, Romania otilia.zvoristeanu@academic.tuiasi.ro

Simona Caraiman
Computer Engineering Department Technical University of Iasi
Iasi, Romania simona.caraiman@academic.tuiasi.ro
Vasile-Ion Manta
Computer Engineering Department Technical University of Iasi
Iasi, Romania
vasile-ion.manta@academic.tuiasi.ro


Abstract—The global number of visually impaired is growing fast due to aging world population. People suffering of severe visual impairments face many dif culties in their daily routine. Pavement holes are a major problem for their navigation and walking. The proposed algorithm detects holes in the sidewalks and roads using a Convolutional Neural Network. Starting from the previously published research, this paper proposes a practical solution for pothole detection used in the Navigation Module of the Sound of Vision Lite (SoV Lite) project. YoloV5s, Mobilenet V1 and Mobilenet V2 Lite were trained on Nvidia RTX using the obtained dataset, then deployed for testing on Nvidia Jetson NX. Due to the mobile platform constraints Mobilenet V1 was chosen to be integrated in SoV Lite. Because objects which are closer have higher detection con dence but also because the visually impaired person wearing the system is interested in the dangers close to him, in practice, we limit the detections of negative obstacles within a range of 8m. By creating a region of interest the run time is also enhanced. For training and experiments we used in-house acquired frames using a ZED 2 Stereo Camera as well as publicly available data and annotated it for the speci c task of detecting potholes and drains in the pavement and streets. The obtained dataset was augmented and made publicly available.
Index Terms—navigation assistant, path hole, visually im-
paired, blind, pothole

Introduction
The World Health Organization estimated in 2011 that 285 million people are suffering from visual impairments, of which
39 million are completely blind. The gures are strongly correlated with the aging world population, 82% of the blind were over 50 years old in 2010. From 1990 to 2002 the population 50 years of age and older increased by 30%, while the world population only increased by 18.5% [2].
Because the risk of blindness and moderate to severe visual impairment (MSVI) grows exponentially with age it is estimated that by the year 2050 there will be roughly 700 million people who are blind or have MSVI [3]. In 2015 12% of the world population was over 60 years old while for 2050 this category is estimated to be 22% of the world population [1].
978-1-6654-0976-6/21/$31.00 ©2021 IEEE
Vision impairment has a major impact at the personal level, blind adults and the ones with MSVI have lower rates of work- force enrollment and higher rates of medical conditions caused by depression. The impact of medical conditions related to sight is even higher to older adults, leading to social isolation, higher risk of fractures caused by falls or walking dif culties. On the other hand, the annual global economic loss caused by lower productivity of visually impaired employees is estimated to 270 billion US$ [4].
Assistive technologies are designed to t user needs and so is the proposed algorithm for the visually impaired persons (VIPs). During the development of the Sound of Vision (SoV) [33], [34] project, a total number of 20 users participated in testing different stages of the system. User feedback led to the necessity of a navigation module capable of avoiding critical situations. The SoV system addresses a few major issues encountered when using the white cane in navigation: detect head level obstacles and signs, alert the user about holes in the ground and be aware of the closest walls. Using the SoV system, the visually impaired can walk without the need to hit the wall with the cane to guide themselves and perceive the building corners. One of the most important features of a Sensory Substitution Device (SSD) for VIPs is danger avoidance, hence the need to detect the potholes (negative obstacles) on the sidewalk.
The paper is organized as follows: In section II, we discuss similar solutions for visually impaired persons assistance. In section III, we present the suitable neural networks for our task detailing the ones chosen to be trained and tested. In section IV, we discuss the used dataset and its augmentation. In sections V and VI we present the results and the integration in the SoV Lite project. Finally, in section VII, we conclude the paper with an outline of future work.
Related work
Different solutions have been proposed for assisted naviga- tion of visually impaired people. The most widely used for navigation are the white cane and the dog guide. In [5] the




409
researchers propose a system combining the smart cane with the Google Tango platform for augmented reality.
Another SSD that also addresses the navigation of blind is described in [6]. An RGB-D camera and an IMU sensor are used, while a lightweight CNN outputs the detected objects. The deployment is done on a smartphone and the human- machine interaction (HMI) is the audio output. Because of the rapid growth of computational power of smartphones and their worldwide adoption, they are potential candidates to host systems designed for the visually impaired [7].
A real-time navigation system for the vision-impaired is proposed in [8]. The accuracy in detecting potholes is over 90%, but the experiments show a limitation caused by the use of a laser pattern which is visible only in the dark.
Another solution using a ResNet backbone [9] is presented by Kwang et al. in [10]. They classify the frame in two categories: pothole and non-pothole. The proposed system uses the camera of a smartphone but it has the inconvenience of deploying it on a laptop. The dataset is composed of 3186 frames: 3028 for training and 159 for testing. Such a method can provide information about the condition of the street/sidewalk but does not localize the holes in order to be avoided by the visually impaired person (VIP).
Jinqiang et al. [11] propose an SSD for guiding VIPs in indoor environments. The input is taken from a depth and an ultrasonic sensor and it is able to output information related to obstacles in front and recommend the walking direction. The device is wearable, the feedback is audio encoded and can be also visually rendered on a pair of AR glasses for weak sighted people.
A solution designed to assist the visually impaired in navigation is presented in [12]. A CNN able to classify street images in path holes and non-path holes is proposed. Two datasets were used, one for each class: Kitti road dataset
[13] for non-path hole and Pothole detection [14] for scenes containing a street with potholes. The main disadvantage of such a solution is that it can only warn about the existence of potholes but does not give any information regarding the localization of the holes, therefore a VIP also needs to use the white cane.
The algorithm proposed by Rui Fan et al. in [15] and
[16] is based on the disparity maps. In order to have a better classi cation of the damaged and undamaged road areas, a dense disparity map is rst computed. Using Otsu's threshold potential undamaged surfaces are extracted and their disparities are modeled using least-squares tting. In the end, the detected potholes are removed from the 3D ground surface.
As we detailed in [19], multiple solutions for object detec- tion and recognition were tested on a Jetson mobile platform. Among the Single Shot Multibox Detectors (SSD) [20] we tested different backbones: MobileNetV1 [21], MobileNetV2
[22] and InceptionV2 [23], all trained on the COCO dataset [24]. Later in 2020 you-only-look-once (YOLO) version 4 [18] and version 5 [17] were released. In terms of runtime, the gures for YoloV4-tiny on the same embedded platform are
signi cantly better, reaching 39 FPS on Jetson Nano and 118 FPS on Jetson Xavier NX.
Mobile pothole detection to assist the VIP in navigation
Regarding the object detection and recognition models, the literature categorizes them into one or two stage detectors. A single-stage model is able to detect objects with no need of a second step. A two-stage detector rst proposes a set called regions of interest and then looks for objects within the selected areas. Useless regions are excluded from the image and the process is easily parallelizable. On the other hand, the one-stage detectors are faster, making them suitable for real- time applications like pothole detection in a moving scene.
Due to the mobile platform constraints, lightweight neural networks were considered. Among those YoloV5 [17], SSD- Mobilenet-v1 and SSD-Mobilenet-v2-lite [32] were trained and tested.
YoloV4 has brought major improvements to the previous iteration. By proposing a new backbone architecture and modifying the neck of the model, the mean average precision (mAP) has been enhanced by 10% and the FPS by 12%. Besides that, the training process has been improved for single GPU architectures [18], like the one used by us - Nvidia Jetson.
YoloV5 [17] is unlike all other Yolo implementations, it uses the PyTorch framework [25] rather than the original Darknet repository of Yolo. The main advantage of this implementation is the variety in terms of architecture complexity: 8 models are available starting with YoloV5s (small) that uses only 7.3M parameters up to YoloV5x6 (extra large) that uses 20 times more parameters. The lightest one is recommended for Nvidia Jetson platforms. Figure 1 illustrates the scale and complexity of the main four YoloV5 architectures.

Fig. 1: YoloV5 main architectures [17]

Single Shot Detector (SSD) [30] is a one-step neural net- work for object detection and recognition. SSD-MobileNet, SSD-Inception are neural architectures recognized for real- time object detection on mobile devices, combining the SSD- 300 neural network [30] with various backbone architectures, for example, MobileNet [21] or Inception [31].
Considering the presented aspects, SSD-Mobilenet-v1, SSD-Mobilenet-v2-lite [32] and YoloV5 [17] have been cho- sen for training and testing on detecting potholes and drains.
Dataset
A few datasets of interest are publicly available, among those we worked with:




410
Dataset 1: Potholes Detection Dataset [27] consists of 665 annotated potholes. The dataset was created and shared by Atikur Rahman Chitholian as part of his thesis. The original dataset did not contain a validation set, later was re-shuf ed into a 70-20-10% train-validation-test split. The dataset is provided in a wide variety of formats for the most common models, YOLO included.
Dataset 2: MIIA Pothole Image Dataset [28] was proposed in 2019 by the Machine Intelligence Institute of Africa for an image classi cation challenge. The goal is to create a machine learning model that can accurately predict the likelihood that an image contains or not potholes. The dataset contains 5676 images of streets in South Africa with and without potholes. The data have been split into 4026 images for training and 1650 for testing. The provided classi cation labels (1-potholes, 0-no potholes) could not be used for the purpose of this paper, therefore a selection of the dataset was manually annotated with bounding boxes for each pothole.
Dataset 3: Another considered dataset [29] is constructed of 90 pothole images without any annotation. It was manually annotated and split into train-val-test subsets.
Dataset 4: SoV Pothole Detection Dataset is the dataset recorded during this project. It is acquired using a ZED 2 stereo camera mounted on the forehead and chest. From the four sequences, were extracted a total of 447 images with annotated potholes and drains. We extracted the left frame, right frame and depth map for each image.
The used dataset is an aggregation of the above presented datasets, resulting a total number of 1490 annotated images with two unique labels: pothole and drain.
TABLE I: Dataset sources

Dataset
Annotated images
MIIA Pothole Image Dataset [28]
288
90 pothole images [29]
90
Potholes Detection Dataset [27]
665
SoV Pothole Detection Dataset
447
Total
1490

Train-val-test split
Finally, the obtained dataset was randomly split by a 70:20:10 train-val-test ratio. For better results, the train-val subset (1341 images) was augmented applying the following six operations:
vertical ip;
horizontal ip;
vertical-horizontal ip;
90º rotation;
average blurring;
raise the hue value.
TABLE II: Train-test Split

Dataset
Annotated images
train-val (90%)
1341
test (10%)
149
train-val-augmented
9387
Table II presents the augmentation of the train-val subset which resulted in 9387 images (1341 original images x 7 operations for each one) with a class distribution of 19250 pothole instances and 2200 annotated drains. The augmen- tation operations are illustrated in Figure 5 and the resulted class distribution after augmentation is plotted in Figure 2. The augmented dataset, providing YOLO and Pascal Voc annota- tion, is publicly available at https://github.com/achireistefan/ Pothole-Detection.

Fig. 2: Class distribution

Experimental results
The testing subset is composed of 149 images with vari- able resolutions, between 180x120 and 1280x720 pixels. All three solutions (ssd-mobilenet-v1, ssd-mobilenet-v2-lite and YoloV5s) have been trained on Nvidia RTX and tested on Nvidia Jetson NX with the same input. In Table III a com- parison in terms of speed is done between the tested neural networks. It can be noticed that the two architectures with a custom design for the mobile platforms Nvidia Jetson (ssd- mobilenet-v1 and ssd-mobilenet-v2-lite) perform much better in terms of frames per second. Another aspect in uencing the network speed is the parameter number. Even the simplest YoloV5 model has 1.7, respectively 2.1 times more parameters than the other two networks.
TABLE III: Runtime comparison

Model
FPS on
Xavier NX
Number of
parameters
SSD Mobilenet v1
167
4.2M
SSD Mobilenet v2 lite
124
3.4M
YoloV5s
20
7.3M

YoloV5s
The lightweight architecture YOLOv5s was trained for 350 epochs and deployed on the last mobile platform from Nvidia, Xavier NX. The evaluation metrics evolution over the training process are showed in Figure 4, for the last epoch the gures are: precision = 0.9917, recall = 0.9809, mAP@0.5 = 0.9895 and mAP0.5:0.95 = 0.8272. Regarding the inference time, on average for input images with variable resolutions, YoloV5s runs at 20 frames per second on Xavier NX.




411

 
(a) Original image (b) Vertical ip





Fig. 3: YoloV5s detection samples

Fig. 4: Evaluation metrics for YoloV5s over 350 training epochs


Mobilenet V1 and Mobilenet V2 lite
The of cial Nvidia implementation of SSD-Mobilenet-v1 and SSD-Mobilenet-v2-lite [32] bene t of an optimized ar- chitecture specially designed for Jetson mobile platforms and have been trained for 600 epochs. On one hand, they run much faster when compared to YoloV5s: Mobilenet-v1 reaches 167 frames per second, while Mobilenet-v2-lite runs at 124fps. On the other hand, as a downside of the network speed, a com- promise is done regarding the overall precision: Mobilenet-v1 reaching 85.0% and Mobilenet-v2-lite 84.1%. A comparison regarding the precision of the three tested networks is done in Table IV.
TABLE IV: Detection precision

Model
Overall
Precision
Per class
Precision
YoloV5s
0.991
-
SSD Mobilenet v1
0.850
pothole: 0.708
drain: 0.993
SSD Mobilenet v2 lite
0.841
pothole: 0.688
drain: 0.995

Figures 3, 6 and 7 show detection samples of the three networks on the testing subset.
Integration in Sound of Vision
The SoV Lite project aims to help visually impaired people to perceive and navigate in almost any type of environment (indoor and outdoor), without the need to use prede ned labels

(c) Horizontal ip (d) Horizontal and vertical ip

(e) Rotated (f) Average blurring

(g) Raised hue
Fig. 5: Dataset augmentation sample


or sensors, under normal lighting conditions: during the day, with natural or arti cial light. This project builds on the results obtained in the Sound of Vision project [34]. SoV Lite aims for an original non-invasive portable device, including hardware and software components, capable of generating a multisen- sory and haptic auditory representation of the environment. This representation is created, updated and transmitted to the visually impaired user continuously and in real-time. The representation allows the user to perceive the environment to some extent, without blocking relevant auditory information from the environment.
Figure 8 presents the SoV Lite system, a portable device that assists people with visual impairments through the anal- ysis and processing of visual information to play important elements in the environment as audible or haptic through a specially designed vest. SoV Lite is a device that works in real-time for both, navigation and environment perception, but also contains modes of use that require a longer time




412
 
Fig. 6: SSD-Mobilenet-v1 detection samples

Fig. 7: SSD-Mobilenet-v2-lite detection samples


to process or send information: detailed scene description, processing text to speech. The SoV Lite device is designed to capture and process 3D sequences of the environment (RGB and depth information) using a ZED 2 camera. The sequences thus taken over are analyzed in real-time to extract the relevant information from the environment.
The system constantly monitors events of interest represent- ing special situations, both of a positive nature (e.g., meeting a social acquaintance) and of a negative nature (e.g., the danger of hitting an object or fall into a pit). If such an event is detected, it will be signaled to the user, in a speci c way, based on its importance and priority.
Perception and navigation of the environment can be a dif cult task for the blind, because sight is the most used sense for the perception of the world. Therefore, it is important that a blind assistance system detects the elements of the environment as accurately as possible and intuitively conveys enough information to the user so that he can easily create a mental map of the world around him. A very important aspect is the accurate detection of potentially dangerous elements, such as potholes or uncovered drains.
Taking all constraints into consideration, the best solution for us able to detect negative obstacles is SSD-Mobilenet-V1, one of the dedicated models for Jetson mobile platform. Even









Fig. 8: SoV Lite Overview

Fig. 9: Comparison between SoV negative obstacle algorithm (top) and the proposed one (bottom)


though it has lower precision when compared to YoloV5s, if needed preprocessing frames before running the model and postprocessing them after are possible thanks to the increased speed of running (167fps).
The proposed solution using neural networks for pothole detection and localization is an alternative for the previously implemented algorithm in SoV for negative obstacle detection
[34] which had false positive detections. An area which is part of the ground is marked as negative obstacle as it can be seen in Figure 9 along with the correct result of the new algorithm. The results for both methods on a dataset composed of 247




413
frames recorded during the SoV project [34] are available at https://github.com/achireistefan/Pothole-Detection. The newly proposed algorithm has reduced drastically the false positive (FP) detections: the old approach had a total number of 194 FP, while out method has only 22.
Another reason for replacing the pothole detection algorithm is the need for a method which is not based on geometric heuristics because the intraclass variety of pothole shapes makes them hard to describe geometrically. Also, the proposed method does not use the ground plane detection, which can also be a source of false positive detections or true negatives. Based on the experimental results it has been observed, as showed in Figures 3, 6 and 7, that the closest the objects, the higher the detection con dence. Considering that a VIP needs to be aware of the dangers close to him, in practice, the proposed solution extracts from the depth map a mask of pixels within a speci c range (e.g. 0 to 8 m) and perform detection within that speci c region of interest. Such an example is shown in Figure 10: false positives and far-away detections are marked in the red rectangle, they are not part of the result
after applying a threshold on depth.
Conclusion and future work
In this paper, we train, deploy and discuss the results of three different neural networks for negative obstacles detection and argument the decision of using Mobilenet V1 with a threshold on distance. There were considered only specially designed neural networks for the mobile platforms with respect to the implementation constraints: deployment on a mobile platform, a minimum frame per second rate, precision, easiness of integration, hardware compatibility, etc.
From a performance perspective, the SoV Lite system must provide a dynamic perception of the environment in accor- dance with the user's movement and environmental changes. The response time from the acquisition, processing and ren- dering of auditory and haptic information must be less than 100 milliseconds. The system aims to reduce this value as much as possible, the goal being to have a system that offers updates with a frequency of at least 30 frames per second.
Future work is also set to collect and annotate more in- stances of potholes in a way that the intraclass variety is better covered. This need is clearly noticed in Table IV, the pothole class precision is much lower due to high difference in pothole shapes and ground texture. Continuous model evaluation are used to improve the proposed network and dataset.
Acknowledgment
This work is supported by Romanian National Authority for Scienti c Research (UEFISCDI), Project PN-III-P2-2.1- PTE-2019-0810/2020: SoV Lite - Natural, accessible and ergonomic audio-haptic sensory substitution for the visually impaired.
References
United Nations, Department of Economic and Social Affairs, Population Division (2015). World Population Prospects: The 2015 revision, key ndings and advance tables. Working Paper No. ESA/P/WP.241.


Fig. 10: Irrelevant detections marked in red (top), 0-8m depth mask (middle), detections within the region of interest (bottom)


World Health Organization, Visual impairment, and blindness: Fact sheet number 282, Aug. 2014. [Online]. Available: http://www.who.int/mediacentre/factsheets/fs282/en/
Ackland P, Resnikoff S, Bourne R. World blindness and visual impair- ment: despite many successes, the problem is growing. Community Eye Health. 2017;30(100):71-73. PMID: 29483748; PMCID: PMC5820628.
World Health Organization, Blindness and vision impairment, Feb. 2021. [Online]. Available: https://www.who.int/en/news- room/factsheets/detail/blindness-and-visual-impairment
B. Li, J. Pablo Munoz, X. Rong, Q. Chen, J. Xiao, Y. Tian, A. Arditi and M. Yousuf, Vision-based Mobile Indoor Assistive Navigation Aid for Blind People
J. Bai, Z. Liu, Y. Lin, Y Li, S. Lian and D.Liu, Wearable Travel Aid for Environment Perception and Navigation of Visually Impaired People,
Electronics, vol. 8, pp.697-724, 2019
N. Martiniello, W. Eisenbarth, C. Lehane, A. Johnson and W. Wittich, Exploring the use of smartphones and tablets among people with visual impairments: Are mainstream devices replacing the use of traditional visual aids?, Assistive Technology, 2019.
A. S. Rao, J. Gubbi, M. Palaniswami and E. Wong, A vision-based system to detect potholes and uneven surfaces for assisting blind people, in Proc. IEEE International Conference on Communications (ICC), Kuala Lumpur, pp. 1-6, 2016




414
Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun, Deep Residual Learning for Image Recognition, cs.CV, 2015
K. E. An, S. W. Lee, S. K. Ryu and D. Seo, Detecting a pothole using deep convolutional neural network models for an adaptive shock observing in a vehicle driving, in Proc. IEEE International Conference on Consumer Electronics (ICCE), Las Vegas, NV, USA, pp. 1-2, 2018
J. Bai, S. Lian, Z. Liu, K. Wang and D. Liu, Smart guiding glasses for visually impaired people in indoor environment, IEEE Transactions on Consumer Electronics, vol. 63, no. 3, pp. 258-266, Aug. 2017
M. M. Islam and M. S. Sadi, "Path Hole Detection to Assist the Visually Impaired People in Navigation," in Proc. International Conference on Electrical Engineering and Information & Communication Technology, Dhaka, in press, 2018.
J. Fritsch, T. Kuhnl, and A. Geiger, "A new performance measure and evaluation benchmark for road detection algorithms," in Proc. IEEE Conf. Intell. Transp. Syst., pp. 1693–1700, 2013.
Pothole Detection Dataset. [Online]. Available: http://people.etf.unsa.ba/ aakagic/pothole detection/, accessed on: Jun. 10, 2018.
R. Fan, U. Ozgunalp, B. Hosking, M. Liu, and I. Pitas, "Pothole detection based on disparity transformation and road surface modeling," IEEE Transactions on Image Processing, vol. 29, pp. 897–908, 2020.
R. Fan and M. Liu, "Road damage detection based on unsupervised disparity map segmentation," IEEE Transactions on Intelligent Trans- portation Systems, 2019.
Glenn Jocher; Alex Stoken; Jirka Borovec; NanoCode012; Ayush Chaurasia; TaoXie; Liu Changyu; Abhiram V; Laughing; tkianai; yxNONG; Adam Hogan; lorenzomammana; AlexWang1900; Jan Hajek; Laurentiu Diaconu; Marc; Yonghye Kwon; oleg; wanghaoyang0106; Yann Defretin; Aditya Lohia; ml5ah; Ben Milanko; Benjamin Fineran; Daniel Khromov; Ding Yiwei; Doug; Durgesh; Francisco Ingham, "YOLOv5", DOI: 10.5281/zenodo.4679653
A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, "YOLOv4: Optimal speed and accuracy of object detection," 2020, arXiv:2004.10934. [On- line]. Available: http://arxiv.org/abs/2004.10934
O. Zvoristeanu, S. D. Achirei, N. A. Botezatu, R. -G. Lupu, A. Burlacu and S. Caraiman, "On Improving Perception for Visually Impaired: Requirements, Research and Practicality," 2020 International Conference on e-Health and Bioengineering (EHB), 2020, pp. 1-4, doi: 10.1109/EHB50910.2020.9280265.
L. Wei, A. Dragomir, E. Dumitru, S. Christian, R. Scott, F. Cheng-Yang,
B. Alexander, SSD: Single Shot MultiBox Detector, 2016
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, Mo- bileNetV2: Inverted Residuals and Linear Bottlenecks, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 4510-4520, doi: 10.1109/CVPR.2018.00474.
H. Andrew, Z. Menglong, C. Bo, K. Dmitry, W. Weijun, W. Tobias,
A. Marco, A. Hartwig, MobileNets: Ef cient Convolutional Neural Networks for Mobile Vision Applications, 2017
C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens and Z. Wojna, "Rethinking the Inception Architecture for Computer Vision," 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, 2016, pp. 2818-2826, doi: 10.1109/CVPR.2016.308.
Lin TY. et al., Microsoft COCO: Common Objects in Context, Computer Vision – ECCV 2014
Paszke, A. et al. Pytorch: an imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 (eds Wallach, H. et al.) 8024–8035 (Neural Information Processing Systems, 2019).
R. Fan, X. Ai and N. Dahnoun, "Road Surface 3D Reconstruction Based on Dense Subpixel Disparity Map Estimation," in IEEE Transactions on Image Processing, vol. 27, no. 6, pp. 3025-3035, June 2018, doi: 10.1109/TIP.2018.2808770.
Atikur Rahman Chitholian, Pothole Dataset, available online: https://github.com/chitholian/Potholes-Detection OR https://public.robo ow.ai/object-detection/pothole
Machine Intelligence Institute of Africa, MIIA Pothole Image Classi - cation Challenge, available online: https://zindi.africa/competitions/miia- pothole-image-classi cation-challenge
Pothole Detection Dataset, online https://people.etf.unsa.ba/ aak- agic/pothole detection/

W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.Fu, and A.
Berg. Ssd: Single shot multibox detector. ECCV 2016
C. Szegedy et al., Going deeper with convolutions, 2015 IEEE Con- ference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, 2015, pp. 1-9, doi: 10.1109/CVPR.2015.7298594.
Nvidia Corp., Jetson-inference DNN Vision Library, available online: https://github.com/dusty-nv/jetson-inference
P. Herghelegiu, A. Burlacu and S. Caraiman, "Robust ground plane de- tection and tracking in stereo sequences using camera orientation," 2016 20th International Conference on System Theory, Control and Comput- ing (ICSTCC), 2016, pp. 514-519, doi: 10.1109/ICSTCC.2016.7790717.
S. Caraiman et al., "Computer Vision for the Visually Impaired: the Sound of Vision System," 2017 IEEE International Conference on Computer Vision Workshops (ICCVW), 2017, pp. 1480-1489, doi: 10.1109/ICCVW.2017.175.
P. Herghelegiu, A. Burlacu and S. Caraiman, "Negative obstacle de- tection for wearable assistive devices for visually impaired," 2017 21st International Conference on System Theory, Control and Computing (ICSTCC), 2017, pp. 564-570, doi: 10.1109/ICSTCC.2017.8107095.




415