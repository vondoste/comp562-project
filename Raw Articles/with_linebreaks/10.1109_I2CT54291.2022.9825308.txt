2022 IEEE 7th International conference for Convergence in Technology (I2CT)
Pune, India. Apr 07-09, 2022

eVision - A technological solution to assist vision impaired in self-navigation.

K.G.Hewagama
Department of Information Technology Specializes in Data Science
Sri Lanka Institute of Information Technology Malabe, Sri Lanka kasun.ghewagama@gmail.com

C.R.Hettiarachchi
Department of Information Technology Specializes in Software Engineering
Sri Lanka Institute of Information Technology Malabe, Sri Lanka charith.rhettiarachchi@gmail.com

A.Karunasena
Department of Information Technology
Sri Lanka Institute of Information Technology Malabe, Sri Lanka anuradha.k@sliit.lk
T.D.Suwandarachchi
Department of Information Technology Specializes in Data Science
Sri Lanka Institute of Information Technology Malabe, Sri Lanka tanisha.suwandarachchi@gmail.com

P.L.D.N.Alwis
Department of Information Technology Specializes in Data Science
Sri Lanka Institute of Information Technology
Malabe, Sri Lanka dnuwan.alwis@gmail.com

K.M.L.P.Weerasinghe
Department of Information Technology
Sri Lanka Institute of Information Technology Malabe, Sri Lanka lokesha.w@sliit.lk



Abstract—Visually impaired people face many dif culties in navigation such as crossing the road, identifying signs and text in indoor and outdoor environments and avoiding obstacles. Even though much research has been done to assist visually impaired people, most methods are unpopular, and almost all visually impaired people still only rely on the white cane. This paper proposes eVision which consists of a mobile app as well as a wearable tool that enables visually impaired people to detect obstacles and objects such as moving vehicles and staircases, identify signs, provide assistance with road crossing and natural scene text recognition, using Convolutional Neural Networks and image processing techniques. The CNN architecture used for object detection was SSD Mobilenet V2, since it provided around 95% accuracy for most objects with good performance on mobile. Mobilenet V2 transfer learning model was used for classi cation of objects, which provided around 94% accuracy. For text detection, the EAST algorithm was used, and the method resulted an accuracy around 98%. From the generated data from models, eVision provides audio feedback to the user using a text to speech(TTS) system.
Index Terms—Convolution Neural Network, Image Processing, Mobile App, Scene Text-recognition, Self- navigation, Visual impairment.

Introduction
Humans primarily rely on vision to know their position, to nd directions in the environment, and to recognize elements in their surroundings [1]. However, according to the WHO (World Health Organization), there are around 1 billion people who suffer from moderate to severe visually impairments
and blindness across the globe [2]. Such people nd many dif culties in self-navigation. In addition, visually impairments severely affects the quality of life of those individual by increasing of depression and anxiety, lowering productivity at work, and lowering the percentage of employability [2].
In self-navigation, visually impaired individuals need as- sistance in several aspects. For example, they would need to locate certain objects, avoid collision and read common signs which helps them with navigation. To help visually impaired people to navigate, there are various Orientation and Mobility (O&M) techniques available, such as using a white cane, analyzing and identifying intersections and traf c patterns, sensory training, etc. [3]. These techniques require long-term training to make them effective [4], also if a person gets visual impairment later in life, it will take a lot of effort to adapt to these techniques. Another improvement for visually impaired navigation is the availability of braille in some public places and making braille in countries such as the USA a requirement by law in some public places [5]. But underdeveloped and developing countries are yet to adapt to those procedures. Even though there are few technological solutions for assisting visu- ally impaired people such as WeWALK (a smart white cane) [6], visually impaired people still rely on O&M techniques for their navigation needs.
In the recent past, advancements in computer vision as well as image processing techniques provide promising ca- pabilities for object detection, which could be leveraged to
978-1-6654-2168-3/22/$31.00 ©2022 IEEE 1
assist self-navigation of visually impaired people. However, existing research conducted in the area of applying Computer Vision for enabling self-navigation of visually impaired people have faced several challenges. This includes identifying risky situations and critical messages on signs in real time with precision and alerting the visually impaired individual on time. Especially, when visually impaired people are navigating in outdoor environment, alerting them when to cross a road and when a vehicle is approaching are critical, but however are not much addressed in existing literature suf ciently.
This research paper presents a technological solution con- sisting of a wearable device and a mobile application(eVision) to assist severely visually impaired and blind individuals in navigating both outdoor and indoor. The proposed solution is an all-inclusive system that helps visually impaired people to navigate within unfamiliar environments using mobile com- puter vision technology. The application is specially focused on assisting visually impaired people to avoid obstacles, nav- igate with con dence in critical areas such as crossroads and reading textual messages on natural surroundings. The system delivers a practical and convenient system to the user such that the user will be able to handle most daily tasks such as road crossing, navigation, and text detection while being an accessorized with a white cane.
The rest of the paper is organized as follows. Section II of this paper will discuss related work to the current research. Next, section III will present the methodology adopted in this research to implement the main functionalities of this system. Then, section IV presents the hardware speci cations of the system. The experimental results obtained are presented in section V, and the presented results will be discussed in Section
VI. Finally, the conclusion is presented in Section VI.
Literature Review
Existing research have focused on several important aspects to be considered in assisting self-navigation of vision impaired individuals. These aspects include obstacle and object detec- tion, and recognition and interpretations of signs and signals. What follows next in this section summarizes such existing work.
M. Sun, P. Ding, J. Song, M. Song, and L. Wang [7] developed a system that detect obstacles and measures the distance from these objects to the user. The system was de- veloped using the Java SDK on a Google Tango mobile device, which includes infrared (IR) to obtain depth data. Using the obtained depth data, a 3D model of the real-world environment is created, where the positions of the obstacles are also located. If it was detected that the user is moving towards an obstacle, then the system will provide an acoustic alert and provide the user with an alternative direction. The experiment results indicate that the system can correctly detect the obstacles while the obstacles placed in different positions, such as the center of the room, leaning against the wall, and hanging in the center of the room and directs the user to another direction correctly to avoid the obstacle. Furthermore, it was found that illumination has no in uence on the system and the [7] system work well
in dark environments as well. However, the system fails to detect obstacles with a scale less than 10 cm3, detect any of the obstacles located at a distance over 2 m and the user is not alerted of the nature of the obstacle which are main drawbacks in this system. Another research work [8] focused on detecting obstacles used a stereo camera to take inputs of the view. The camera captured and detected potential obstructions in 3D. The research provides a framework which monitors camera frames through an algorithm for surface level estimation technique based on the RANSAC plus ltering method. A polar grid representation is introduced to identify moving obstacles in the scenario, and users are noti ed using beep sounds of different frequencies. Triangular and square shaped obstacles are used in this research and the R-CNN was able to recognize triangle blocks with precision, recall, and accuracy 96.88%, 100%, and 98.44% respectively. Even though the results of the above research shows higher values of accuracy, this application is not applicable for outdoor environments due to higher delays in calculations and the method is applicable for small objects rather than large objects such as vehicles.
Finding a chair through a wearable vision-based feedback system have been performed in the research [9]. The system includes a belt which is embedded with motors and a camera. The task of nding a chair includes checking whether there are objects to collide in front of the user and steering toward a chair. The user could start with a random walk until receiving a vibration signal from a suitable motor according to the direc- tion of the identi ed chair. The motor will stop the vibration when the user turns toward the chair.The main drawback in this system is that the user has to carry considerable weight in order to nd directions to navigate.
Akhilesh A, M.S. Panse, and, Shrugal Varde [10] proposed a scene text recognition system to detect and recognize text from surrounding visually impaired people. The authors have used pre-processing steps such as image grayscaling, median ltering, and binarization to make the image black and white, and then extracted the white components (which is the text) using connected-component implemented with MATLAB, the system provides the best accuracy for the graphical text images and decent accuracy for scene text images. Finally, even though the goal of the research is to develop a text recognition system that can be incorporated into Electronic travel aid devices, the integration of this system with traveling aids is not well explained. Authors in paper [11] proposes text detection and recognition on navigational signboards using Optical Character Recognition (OCR) and image processing. The pro- posed system uses android devices and network connections to provide audio feedback of the recognized text. The system uses image processing techniques to make the text highlighted from the environment, and then uses the tesseract library for OCR. Next, Content-Free Grammar (CRG) algorithm was used to handle spell correction before TTS was used to provide audio feedback to the user. Although the system works for graphical and basic natural scene images, the system nds dif culties with images that have texts that are skewed, angled, or further away.
Researchers from the USA [12], proposed a system to detect traf c lights for outdoor navigation. The proposed method includes GPS to locate the pedestrian crossing, cloud server and OpenCV to process the video feed that comes from the images and then outputs a voice command. The paper [13] develops a prototype system for outdoor navigation with traf c light detector and zebra crossing detector using Real-Time Kinematic (RTK) and existing information regarding traf c lights, timers of those traf c lights, and zebra crossings. The papers [14][15][16] also proposed systems to detect only the pedestrian crossing by image processing techniques.Despite the fact that these systems have an accuracy rate of over 80%, they lack an ef cient and reliable approach that analyzes traf c signals, zebra crossings, moving vehicles, and traf c signs associated with crossing roadways.
Although there is much research conducted on assisting vision-impaired individuals to navigate, there is no com- prehensive solution that considers both indoor and outdoor navigation. Furthermore, real-time detection of vehicles and pedestrian crossings and alerting of users, and portability of the devices have been found as issues in existing research. Therefore, eVision proposes a small, portable, and hands-free wearable comprehensive tool for visually impaired persons to navigate both indoor and outdoor.
Methodology
E-vision consists of four main components to assist visually impaired individuals to assist in self-navigation. Those four components are as follows :
 
 Chair, Table, Bed, Staircase) 
Door detection for sign detection and text reader 
Outdoor Navigation obstacle detection (Vehicles, Bus 
 Stop signs) 
Road Crossing (Traf c light detection, Zebra crossing 
 detection) 
What follows next in this section, discusses how the above four components are developed
Data gathering and Pre-processing
The images necessary for training models were gathered by using resources such as Google, Kaggle and by visiting public places to capture images and video footage. Then processed the captured raw data by capturing relevant images from videos and removing redundant and blurred images. Then labeled them and selected around 250-300 for each class (Object). Finally, the preprocessed images were divided into 70% as training data and 30% as testing data.
Target Detection
Detecting the targets such as chairs, staircases, vehicles and crossroads accurately and ef ciently is critical to ensure the safety of the visually impaired people. Therefore, for identi cation of targets several types of models such as SSD Mobilenet V2, Ef cientDet, and Yolo were used.
Statistic of ef ciency, performance, accuracy comparing with YOLO V3 and Ef cientDet D0 Based on the support of TensorFlow-Lite, SSD Mobilenet V2 model was selected for further development of the mobile application.
The ef ciency of the SSD Mobilenet V2 model was im- proved by tuning the parameters of number of classes, batch size, checkpoint type, learning rate and required le paths.
Classi cation of objects
Classifying detected objects is extremely important to pro- vide descriptive information about the objects to the user. In eVision, classi cation is done for scenarios such as identifying male and female washroom signs and detecting whether a chair is empty or not. For the classi cation, a transfer learning CNN was used with the Mobilenet V2 architecture, which is pre- trained on the imageNet dataset.
To train the model rst annotated the images then, aug- mented them to generate a dataset of 3 times the original dataset and divided them to 70%, 20%, and 10% as training, validation, and test images. To feed the model, resized the images to 160×160 pixels. CNN Mobilenet V2 model was used as the base CNN model. The features were then extracted by freezing the convolutional base model. Then added an average pooling layer to convert the features to a single 1280-element vector per image. Next added a single dense layer without an activation function since it automatically acts as a logistic regression function. The developed model was compiled with "BinaryCrossentropy" for classi cation with 2 classes. Finally, the dataset was loaded and the developed model was trained for 50 epochs.
Once the target detection model identi es a staircase, the classi cation model was used to nd whether the staircase is on upwards or downwards. Chairs also can be classi ed as empty or non-empty according to the user request.
Identi cation and recognition of text
eVision let the user capture an image of any natural scene (including doors and notice boards) and it automatically recognizes the text available (in English language) and provide the user the audio output of the recognized text.

Pre-processing and preparation of the images: As the rst step of pre-processing, the images were resized to the dimensions of 640×320 pixels. This is because the algorithms used for text detection expect the images to be in multiples of 32 pixels. Then, gray scaling was applied to the images with Gaussian blurring to remove the presence of high-frequency noise. Then, several thresholds and adaptive thresholds were performed with several more blurring and ltering techniques such as medianBlur, GaussianBlur, and bilateral lter. During the pre-processing, the thresholding is done to minimize the amount of background noise in the image around the text.
Scene text detection using EAST algorithm: The pro- posed system contains a fully connected neural network that is trained to predict the existence of text instances of the geometries [17].The EAST algorithm outperforms most of the
state-of-the-art text detection methods in terms of accuracy and performance and therefore, the EAST algorithm was used. The EAST algorithm frozen graph is used for detecting the text available in the background. Prior to using the EAST algo- rithm, several speci c image processing tasks were performed. After resizing, mean subtraction was performed on the images [18]. Finally, the 2 layers which are the "feature- fusionSigmoid" and the "feature-fusion concat3" are de ned in a list and provided to the frozen graph with the image to predict any text available. The EAST algorithm then provides the positions and the geometries of each word as x and y
coordinates.
Reading detected text: For text recognition, the Easy- OCR library from python was used.
To read the detected text, only the text areas detected using the EAST algorithm are provided to easyOCR as input. Then easyOCR reads text from the image and outputs a list containing the words as string values and their con dence level. These values are then provided to the client and nally, the voice output of detected texts are provided using text-to- speech.
Outdoor navigation obstacle detection
One of the main features of EVision is to provide a visually impaired person with much independence and safety, by estimating the distance to detected obstacles such as vehicles and giving warnings if the object may cause a hazard to the user. For the above purpose, the system can calculate the distance to the detected object relative to the user when an object is located in outdoor. Evision could locate objects up to 10 meters. If the distance to the obstacle is lower than 3 meters and the user directly in-front of the obstacle, the system immediately gives an audio warning to the user.

(a) Beyond Collision Dis- (b) Within Collision Dis-


Fig. 2: How to detect the frame of the vehicle


system approximately calculate distance by rounding off the nearest 1-meter distance.


apxDistance = round(((1 (boxes[0][i][3] boxes[0][i][1]))), 1)

Crossing the Road
In this component of the eVision system, the main focus is to make visually impaired people more secure and independent by aiding the crossing of roads while navigating in outdoor. The component processes visual feed which comes from the wearable camera or mobile camera with the trained models and gives a voice instruction to the user. This component consists of four different parts.
Detecting the traf c signs that are related to crossing 
 roads 
Detecting the traf c light 
Detecting the status of the traf c light 
Detecting the cross roads 

(a) road crossing (b) school crossing (c) traf c light ahead
tance
tance
Fig. 1: Collision Distance
Fig. 3: Crossroad Related Signs

The system only detects 3 traf c signs that are related to
When an object was detected, a relevant bounding box will be generated within that frame. Each bounding box gives a score which represent the level of con dence for each of the detected object.Score is shown on the resulted image, together with the class label. From those boxes can take the pixels X×Y of the detected object relative to the whole frame of the screen as shown in the Fig 2. To get the values continuously, boxes are added into a recursive array and the distance with detected object is output on real time.
Identi cation number of the detected obstacle is given by 'i' and pass that ID to check whether the distance is need to calculate or not. System already speci ed the objects that need to take the distance. Once the speci ed object is detected, the
crossing roads (Fig 3). The system analyzes the surroundings when the user wants to cross the road and if above signs are detected, users will be noti ed via audio output based on the sign content.

Furthermore, the system detects traf c lights, status of the traf c lights and cross roads. To ensure the security of the user, the system does calculations based on the detections and then noti es the user regarding the crossing road.(Fig 4)

In this research it is assumed that all the vehicles follows the traf c lights and all the traf c lights working properly











Fig. 4: CrossRoad Diagram 


Hardware Components of the Prototype
Fig 5 contains the prototype of the wearable tool, and TABLE I contains the hardware speci cations of that prototype system.
TABLE I: Hardware Speci cations

Hardware
Features
Mobile
Android 7+, in built camera
Raspberry Pi Camera
4 mm focal length, 600 view
angle and 3.0 MP
Raspberry Pi 3 Model B
CPU: 1.2 GHz quad-core, 1
GB LPDDR2-900 SDRAM and
32 GB storage
Headset
Bluetooth


Fig. 5: Prototype tool

Results & Discussion
Obstacle Detection Accuracy
This section is focused on the predictive performance of SSD Mobilenet V2 approach in order to compare the results of obstacle identi cation. After training data, the model was evaluated to measure the accuracy. Following TABLE II con- tains few different types of classes with the generated accuracy.
TABLE II: Model accuracy

Detection
Accuracy
Water Filter
90.0%
Door
93.5%
Car
86.67%
Traf c Light
81.2%
Fig 6 illustrates the plot of classi cation loss and localiza- tion loss over 50000 training epochs, which were achieved from the nalized SSD Mobilenet V2 model.
Fig. 6: SSD Mobilenet V2 model loss



Classi cation Accuracy


From the trained Mobilenet V2 model for classi cation, a validation accuracy of around 87.7% was recorded from the trained Mobilenet V2 model for classi cation, and the pre- trained model was ne-tuned according to the dataset in order to increase the accuracy of the mode further. When ne-tuning the top layers (after the 100th layer, Mobilenet V2) were made trainable. This method is only effective after training the model with all the pre-trained layers frozen [19]. After ne-tuning the model, the validation accuracy was increased to 93% approximately and the validation loss was reduced to around 0.16. Fig. 7 shows the nalized, ne-tuned model summary.


Fig. 7: Finalized model summary



Fig 8(a) and Fig 8(b) depicts the plot of the performance accuracy graph and performance loss graph before and after the model ne-tuning respectively.


(a) Before ne-tuning (b) After ne-tuning
Fig. 8: Model performance accuracy and performance loss
Distance Calculation Accuracy

Fig. 9: Actual vs Detected distance

The distance from 1 m to 10 m can calculated by the system. The diagram above Fig:9 shows experimental results on the distance and the real distance to the obstacle.
Text Recognition
This section demonstrates the accuracy of text detection and recognition. 30 images from the ICDAR dataset and random google images are selected for better comparison. Selected images are categorized into 3 types; (i) graphical images (Dif culty: Low), (ii) scene text images with close range text (Dif culty: Medium), (iii) Far away, slightly rotated, low resolution, and cursive text images (Dif culty: High) (TABLE III).
TABLE III: Text accuracy results

Detection dif culty
Text Identi cation
Text Recognition
Low (5 images)
96%
100%
Medium (10 images)
98%
98%
High
86.3%
81%
This research paper is embedded with four different criteria to ful l a single objective for the decades. Obstacle detection, text recognition, distance calculation and crossroad navigation are the components of the research. In the last decade, most researchers tried to solve this problem with different types of approaches by selecting only one or two components from the above-mentioned. Even though our system is still a prototype, it has been tested in real-world situations with visually impaired users, demonstrating the effectiveness of the approach. The overall system gains above 90% of accuracy. The system ful lls most of the common problems of visually handicapped people by advancing them with new and user- friendly technology. There is no delay between input and output due to the computational work calculated within the local environment. The user can easily hang around anywhere without noticing others due to the tiny mass of the system.
Conclusion
In this paper, the proposed system is capable of navigating visually impaired people through indoor and outdoor environ- ments using a mobile application and a wearable tool. As
can be seen in the previous result section, the experimental results demonstrated that the proposed system delivers faster and reliable outputs.
In future work, the system will be enhanced to reduce response time and to increase the accurate of results. Fur- thermore, It will be upgraded with night vision and new objects/obstacles for detection.
References
Real and Araujo. "Navigation Systems for the Blind and Vi- sually Impaired: Past Work, Challenges, and Open Problems". en. In: Sensors 19.15 (Aug. 2019), p. 3404. ISSN: 1424-8220.
DOI: 10.3390/s19153404. URL: https://www.mdpi.com/1424- 8220/19/15/3404 (visited on 07/28/2021).
Vision impairment and blindness. en. URL: https://www.who. int / news - room / fact - sheets / detail / blindness - and - visual - impairment (visited on 07/28/2021).
Orientation and Mobility Skills. en-US. URL: https : / / visionaware . org / everyday - living / essential - skills / an - introduction- to- orientation- and- mobility- skills/ (visited on 08/19/2021).
Grace P. Soong, Jan E. Lovie-Kitchin, and Brian Brown. "Does Mobility Performance of Visually Impaired Adults Improve Immediately After Orientation and Mobility Train- ing?:" en. In: Optometry and Vision Science 78.9 (Sept. 2001),
pp. 657–666. ISSN: 1040-5488. DOI: 10 . 1097 / 00006324 -
200109000- 00011. URL: http://journals.lww.com/00006324- 200109000-00011 (visited on 08/19/2021).
Where is Braille Required by Law? en-US. Jan. 2019. URL: https://vocalinkglobal.com/braille- law- requirements/ (visited on 07/18/2021).
Logan Kugler. "Technologies for the visually impaired". en. In: Communications of the ACM 63.12 (Nov. 2020), pp. 15–
17. ISSN: 0001-0782, 1557-7317. DOI: 10 . 1145 / 3427936.
URL: https:// dl . acm. org/ doi/ 10 . 1145 / 3427936 (visited on 08/19/2021).
Minghui Sun et al. ""Watch Your Step": Precise Obstacle Detection and Navigation for Mobile Users Through Their Mobile Service". In: IEEE Access 7 (2019), pp. 66731–66738. ISSN: 2169-3536. DOI: 10 . 1109 / ACCESS . 2019 . 2915552.
URL: https://ieeexplore.ieee.org/document/8710624/ (visited on 07/20/2021).
Yi-Chun Du et al. "Stereo Vision-Based Object Recognition and Manipulation by Regions with Convolutional Neural Network". en. In: Electronics 9.2 (Jan. 2020), p. 210. ISSN: 2079-9292. DOI: 10 . 3390 / electronics9020210. URL: https :
//www.mdpi.com/2079-9292/9/2/210 (visited on 07/20/2021).
Hsueh-Cheng Wang et al. "Enabling independent navigation for visually impaired people through a wearable vision-based feedback system". In: 2017 IEEE International Conference on Robotics and Automation (ICRA). Singapore, Singapore: IEEE, May 2017, pp. 6533–6540. ISBN: 9781509046331. DOI:
10.1109/ICRA.2017.7989772. URL: http://ieeexplore.ieee.org/ document/7989772/ (visited on 08/01/2021).
Akhilesh A. Panchal, Shrugal Varde, and M. S. Panse. "Char- acter detection and recognition system for visually impaired people". In: 2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technol- ogy (RTEICT). Bangalore, India: IEEE, May 2016, pp. 1492– 1496. ISBN: 9781509007745. DOI: 10 . 1109 / RTEICT. 2016 .
7808080. URL: http://ieeexplore.ieee.org/document/7808080/ (visited on 07/18/2021).
Hao Jiang et al. "Computer vision and text recognition for as- sisting visually impaired people using Android smartphone". In: 2017 IEEE International Conference on Electro Informa- tion Technology (EIT). Lincoln, NE, USA: IEEE, May 2017,
pp. 350–353. ISBN: 9781509047673. DOI: 10.1109/EIT.2017.
8053384. URL: http://ieeexplore.ieee.org/document/8053384/ (visited on 07/18/2021).
Pelin Angin, Bharat Bhargava, and Sumi Helal. "A mobile- cloud collaborative traf c lights detector for blind navigation". In: 2010 Eleventh International Conference on Mobile Data Management. IEEE. 2010, pp. 396–401.
Anna N Lapyko, Li-Ping Tung, and Bao-Shuh Paul Lin. "A cloud-based outdoor assistive navigation system for the blind and visually impaired". In: 2014 7th IFIP Wireless and Mobile Networking Conference (WMNC). IEEE. 2014, pp. 1–8.
Mohammad Shorif Uddin and Tadayoshi Shioyama. "Bipo- larity and projective invariant-based zebra-crossing detection for the visually impaired". In: 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)-Workshops. IEEE. 2005, pp. 22–22.
Md Masud Haider et al. "Zebra Crosswalk Region Detec- tion and Localization Based on Deep Convolutional Neu- ral Network". In: 2019 IEEE International Conference on Robotics, Automation, Arti cial-intelligence and Internet-of- Things (RAAICON). IEEE, pp. 93–97.
Jiahao Zhong et al. "Improved U-net for Zebra-crossing Image Segmentation". In: 2020 IEEE 6th International Conference on Computer and Communications (ICCC). IEEE. 2020,
pp. 388–393.
Xinyu Zhou et al. EAST: An Ef cient and Accurate Scene Text Detector. 2017. arXiv: 1704.03155 [cs.CV].
Marijan Milovec. Face detection with OpenCV and Deep
Learning from image-part 1. en. Aug. 2018. URL: https : / / becominghuman. ai/ face- detection- with- opencv- and- deep- learning-90b84735f421 (visited on 07/16/2021).
Keras Team. Keras documentation: Transfer learning & ne- tuning. en. URL: https : / / keras . io / guides / transfer learning/ (visited on 08/06/2021).